# Regresi√≥n Lineal

## Introducci√≥n

La **regresi√≥n lineal** es una de las t√©cnicas estad√≠sticas m√°s fundamentales y ampliamente utilizadas. Modela la relaci√≥n entre una variable dependiente (Y) y una o m√°s variables independientes (X) mediante una l√≠nea recta.

## Regresi√≥n Lineal Simple

Modelo con una sola variable independiente:

```
Y = Œ≤‚ÇÄ + Œ≤‚ÇÅX + Œµ
```

Donde:
- **Y**: Variable dependiente (respuesta)
- **X**: Variable independiente (predictora)
- **Œ≤‚ÇÄ**: Intercepto (valor de Y cuando X = 0)
- **Œ≤‚ÇÅ**: Pendiente (cambio en Y por unidad de X)
- **Œµ**: Error aleatorio

## Ecuaci√≥n de la Recta

La l√≠nea de mejor ajuste se expresa como:

```
≈∂ = b‚ÇÄ + b‚ÇÅX
```

Donde:
- **≈∂**: Valor predicho de Y
- **b‚ÇÄ**: Estimaci√≥n del intercepto
- **b‚ÇÅ**: Estimaci√≥n de la pendiente

## M√©todo de M√≠nimos Cuadrados

El m√©todo m√°s com√∫n para encontrar la mejor l√≠nea.

### Objetivo
Minimizar la suma de los cuadrados de los residuos:

```
SSE = Œ£(yi - ≈∑i)¬≤
```

Donde:
- **yi**: Valor observado
- **≈∑i**: Valor predicho
- **Residuo**: ei = yi - ≈∑i

### F√≥rmulas para los Coeficientes

**Pendiente**:
```
b‚ÇÅ = Œ£[(xi - xÃÑ)(yi - »≥)] / Œ£(xi - xÃÑ)¬≤
```

**Intercepto**:
```
b‚ÇÄ = »≥ - b‚ÇÅxÃÑ
```

## Ejemplo Pr√°ctico

**Problema**: Predecir ventas bas√°ndose en gastos en publicidad.

**Datos**:
| Publicidad (X) | Ventas (Y) |
|----------------|------------|
| 1              | 2          |
| 2              | 3          |
| 3              | 5          |
| 4              | 4          |
| 5              | 6          |

**C√°lculos**:
```
xÃÑ = 3, »≥ = 4

b‚ÇÅ = [(1-3)(2-4) + (2-3)(3-4) + ... + (5-3)(6-4)] / [(1-3)¬≤ + (2-3)¬≤ + ... + (5-3)¬≤]
b‚ÇÅ = 10 / 10 = 1

b‚ÇÄ = 4 - 1(3) = 1
```

**Modelo**:
```
Ventas = 1 + 1 √ó Publicidad
```

**Predicci√≥n**: Si gastamos 6 en publicidad:
```
≈∂ = 1 + 1(6) = 7 unidades vendidas
```

## Coeficiente de Determinaci√≥n (R¬≤)

Mide qu√© tan bien el modelo explica la variabilidad en Y.

```
R¬≤ = 1 - (SSE / SST)
```

Donde:
- **SSE**: Suma de cuadrados de errores
- **SST**: Suma total de cuadrados

### Interpretaci√≥n de R¬≤

- **R¬≤ = 0**: El modelo no explica nada
- **R¬≤ = 0.5**: El modelo explica 50% de la variabilidad
- **R¬≤ = 1**: El modelo explica perfectamente los datos

### Ejemplo
Si R¬≤ = 0.75:
- 75% de la variaci√≥n en ventas se explica por publicidad
- 25% se debe a otros factores

## R¬≤ Ajustado

Para regresi√≥n m√∫ltiple, penaliza por agregar variables:

```
R¬≤_adj = 1 - [(1 - R¬≤)(n - 1) / (n - k - 1)]
```

Donde:
- **n**: N√∫mero de observaciones
- **k**: N√∫mero de predictores

## Supuestos del Modelo

### 1. Linealidad
La relaci√≥n entre X e Y es lineal.

**Verificaci√≥n**: Scatter plot de X vs Y

### 2. Independencia
Las observaciones son independientes entre s√≠.

**Verificaci√≥n**: Conocimiento del dise√±o del estudio

### 3. Homoscedasticidad
La varianza de los errores es constante.

**Verificaci√≥n**: Plot de residuos vs valores ajustados

### 4. Normalidad de Errores
Los residuos siguen una distribuci√≥n normal.

**Verificaci√≥n**: Q-Q plot de residuos

### 5. No Multicolinealidad
(Solo para regresi√≥n m√∫ltiple) Las variables independientes no est√°n altamente correlacionadas.

**Verificaci√≥n**: VIF (Variance Inflation Factor)

## An√°lisis de Residuos

Los **residuos** son la diferencia entre valores observados y predichos:

```
e = y - ≈∑
```

### Gr√°ficos Importantes

**1. Residuos vs Valores Ajustados**
- Patr√≥n aleatorio ‚Üí Buen modelo
- Patr√≥n en forma de embudo ‚Üí Heteroscedasticidad
- Patr√≥n curvo ‚Üí No linealidad

**2. Q-Q Plot**
- Puntos en l√≠nea diagonal ‚Üí Normalidad
- Desviaciones ‚Üí Violaci√≥n de normalidad

**3. Residuos vs Orden**
- Detecta autocorrelaci√≥n en series de tiempo

## Regresi√≥n Lineal M√∫ltiple

Modelo con m√∫ltiples predictores:

```
Y = Œ≤‚ÇÄ + Œ≤‚ÇÅX‚ÇÅ + Œ≤‚ÇÇX‚ÇÇ + ... + Œ≤‚ÇñX‚Çñ + Œµ
```

### Ejemplo
Predecir precio de casa:

```
Precio = Œ≤‚ÇÄ + Œ≤‚ÇÅ(√Årea) + Œ≤‚ÇÇ(Habitaciones) + Œ≤‚ÇÉ(Edad) + Œµ
```

### Interpretaci√≥n de Coeficientes
**Œ≤‚ÇÅ**: Cambio en Y cuando X‚ÇÅ aumenta 1 unidad, **manteniendo constantes** las dem√°s variables.

## Inferencia en Regresi√≥n

### Prueba de Significancia de Œ≤‚ÇÅ

**Hip√≥tesis**:
- H‚ÇÄ: Œ≤‚ÇÅ = 0 (X no tiene efecto en Y)
- H‚ÇÅ: Œ≤‚ÇÅ ‚â† 0 (X tiene efecto en Y)

**Estad√≠stico t**:
```
t = b‚ÇÅ / SE(b‚ÇÅ)
```

Si |t| es grande (p < 0.05), rechazamos H‚ÇÄ.

### Intervalo de Confianza para Œ≤‚ÇÅ

```
IC‚Çâ‚ÇÖ% = b‚ÇÅ ¬± t* √ó SE(b‚ÇÅ)
```

## Predicci√≥n

### Predicci√≥n Puntual
Usar la ecuaci√≥n directamente:
```
≈∂ = b‚ÇÄ + b‚ÇÅX
```

### Intervalo de Confianza para la Media
Rango donde esperamos que est√© la **media** de Y para un valor dado de X.

### Intervalo de Predicci√≥n
Rango donde esperamos que est√© un **valor individual** de Y.

**Nota**: El intervalo de predicci√≥n es m√°s ancho que el de confianza.

## Limitaciones

### 1. Extrapolaci√≥n
No predecir fuera del rango de datos observados.

**Ejemplo**: Si tenemos datos de publicidad entre $1-10K, no predecir para $100K.

### 2. Correlaci√≥n ‚â† Causalidad
La regresi√≥n muestra asociaci√≥n, no necesariamente causa-efecto.

### 3. Outliers
Puntos extremos pueden distorsionar la l√≠nea.

**Soluci√≥n**: Identificar y evaluar outliers

### 4. Overfitting
Modelo muy complejo que se ajusta al ruido.

**Soluci√≥n**: Validaci√≥n cruzada, regularizaci√≥n

## Transformaciones

Cuando los supuestos no se cumplen:

### Transformaci√≥n Logar√≠tmica
```
log(Y) = Œ≤‚ÇÄ + Œ≤‚ÇÅX
```

√ötil para:
- Relaciones exponenciales
- Estabilizar varianza

### Transformaci√≥n Polinomial
```
Y = Œ≤‚ÇÄ + Œ≤‚ÇÅX + Œ≤‚ÇÇX¬≤ + Œ≤‚ÇÉX¬≥
```

Captura relaciones no lineales.

## Regularizaci√≥n

T√©cnicas para prevenir overfitting:

### Ridge Regression (L2)
Penaliza coeficientes grandes:
```
Minimizar: SSE + ŒªŒ£Œ≤¬≤
```

### Lasso Regression (L1)
Puede reducir coeficientes a cero:
```
Minimizar: SSE + ŒªŒ£|Œ≤|
```

### Elastic Net
Combina Ridge y Lasso.

## Aplicaciones

### En Negocios
- Predicci√≥n de ventas
- An√°lisis de precios
- Forecasting financiero

### En Ciencias Sociales
- Estudios de salarios
- An√°lisis educativo
- Investigaci√≥n psicol√≥gica

### En Machine Learning
- Baseline model
- Feature importance
- Interpretabilidad

## Evaluaci√≥n del Modelo

### M√©tricas Comunes

**MSE (Mean Squared Error)**:
```
MSE = Œ£(yi - ≈∑i)¬≤ / n
```

**RMSE (Root Mean Squared Error)**:
```
RMSE = ‚àöMSE
```

**MAE (Mean Absolute Error)**:
```
MAE = Œ£|yi - ≈∑i| / n
```

## Validaci√≥n

### Train-Test Split
- Entrenar en 70-80% de datos
- Evaluar en 20-30% restante

### Cross-Validation
- K-fold cross-validation
- Leave-one-out CV

## Conclusi√≥n

La regresi√≥n lineal es:
- ‚úÖ Simple e interpretable
- ‚úÖ R√°pida de entrenar
- ‚úÖ Buena como baseline
- ‚úÖ √ötil para inferencia

Pero requiere:
- ‚ö†Ô∏è Verificar supuestos
- ‚ö†Ô∏è Cuidado con outliers
- ‚ö†Ô∏è No asumir causalidad

## Juegos Relacionados

üéÆ [Regresi√≥n Lineal Builder](/game/linear-regression) - Ajusta l√≠neas y mejora tu R¬≤

---

*Siguiente: [Correlaci√≥n vs Causalidad](/wiki/correlacion-causalidad)*
