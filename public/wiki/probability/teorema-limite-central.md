# Teorema del L√≠mite Central

## Introducci√≥n

El **Teorema del L√≠mite Central** (TLC) es uno de los resultados m√°s importantes y poderosos de la probabilidad y estad√≠stica. Establece que la suma (o promedio) de un gran n√∫mero de variables aleatorias independientes tiende a seguir una distribuci√≥n normal, sin importar cu√°l sea la distribuci√≥n original de las variables. Este teorema es la base de gran parte de la inferencia estad√≠stica moderna.

## Conceptos Fundamentales

### Enunciado del Teorema

Si X‚ÇÅ, X‚ÇÇ, ..., X‚Çô son variables aleatorias **independientes e id√©nticamente distribuidas** (i.i.d.) con:
- Media Œº (finita)
- Varianza œÉ¬≤ (finita)

Entonces, cuando n es grande, la suma o promedio de estas variables se aproxima a una distribuci√≥n normal:

```
XÃÑ = (X‚ÇÅ + X‚ÇÇ + ... + X‚Çô) / n

XÃÑ ~ Normal(Œº, œÉ¬≤/n)  aproximadamente
```

O equivalentemente, la variable estandarizada:
```
Z = (XÃÑ - Œº) / (œÉ/‚àön) ~ Normal(0, 1)
```

### Requisitos del TLC

1. **Independencia**: Las variables deben ser independientes
2. **Tama√±o de muestra**: n debe ser "suficientemente grande" (t√≠picamente n ‚â• 30)
3. **Momentos finitos**: Media y varianza deben existir y ser finitas

**Nota:** No se requiere que las variables originales sean normales.

## ¬øPor Qu√© es Importante?

### 1. Universalidad
Funciona con CUALQUIER distribuci√≥n original (uniforme, exponencial, binomial, etc.)

### 2. Justifica la Distribuci√≥n Normal
Explica por qu√© tantos fen√≥menos naturales siguen una distribuci√≥n normal (son sumas de muchos efectos peque√±os e independientes)

### 3. Base de la Inferencia Estad√≠stica
- Intervalos de confianza
- Pruebas de hip√≥tesis
- Estimaci√≥n de par√°metros

### 4. Simplifica C√°lculos
Permite usar la distribuci√≥n normal para aproximar otras distribuciones complejas

## Ejemplos Pr√°cticos

### Ejemplo 1: Lanzamiento de Dados

Lanzar un dado una vez: distribuci√≥n uniforme (1, 2, 3, 4, 5, 6)
- Œº = 3.5
- œÉ¬≤ = 35/12 ‚âà 2.917

**Promedio de 2 dados:** Ya comienza a verse m√°s sim√©trica

**Promedio de 30 dados:** 
```
XÃÑ ~ Normal(3.5, 2.917/30) = Normal(3.5, 0.0972)
œÉ_XÃÑ = ‚àö0.0972 ‚âà 0.312
```

La distribuci√≥n del promedio es aproximadamente normal, ¬°aunque cada dado individual no lo es!

### Ejemplo 2: Control de Calidad

Una m√°quina produce tornillos con peso promedio Œº = 50g y œÉ = 2g (distribuci√≥n no especificada).

Si tomamos muestras de 36 tornillos:
```
XÃÑ ~ Normal(50, 2¬≤/36) = Normal(50, 0.111)
œÉ_XÃÑ = 2/‚àö36 = 0.333g
```

**Probabilidad de que el promedio est√© entre 49.5g y 50.5g:**
```
Z‚ÇÅ = (49.5 - 50) / 0.333 = -1.50
Z‚ÇÇ = (50.5 - 50) / 0.333 = 1.50

P(49.5 < XÃÑ < 50.5) = P(-1.50 < Z < 1.50) ‚âà 0.866 = 86.6%
```

### Ejemplo 3: Encuestas y Muestreo

En una elecci√≥n, el 45% de los votantes prefieren al candidato A. Si encuestamos a 1000 personas aleatoriamente:

```
n = 1000, p = 0.45
Œº = np = 450
œÉ = ‚àö(np(1-p)) = ‚àö(1000 √ó 0.45 √ó 0.55) = 15.73

Proporci√≥n muestral: pÃÇ ~ Normal(0.45, 0.45√ó0.55/1000)
œÉ_pÃÇ = ‚àö(0.45√ó0.55/1000) = 0.0157 ‚âà 1.57%
```

El 95% de las encuestas dar√°n resultados entre:
```
0.45 ¬± 1.96 √ó 0.0157 = [0.419, 0.481] o [41.9%, 48.1%]
```

### Ejemplo 4: Suma de Variables Uniformes

X‚ÇÅ, X‚ÇÇ, ..., X‚ÇÅ‚ÇÇ ~ Uniforme(0, 1) independientes

```
Para cada X·µ¢:
Œº = 0.5
œÉ¬≤ = 1/12

Para la suma S = X‚ÇÅ + ... + X‚ÇÅ‚ÇÇ:
E[S] = 12 √ó 0.5 = 6
Var(S) = 12 √ó (1/12) = 1
œÉ_S = 1

Por el TLC: S ~ Normal(6, 1) aproximadamente
```

Este es un m√©todo hist√≥rico para generar n√∫meros pseudo-normales.

## Convergencia seg√∫n n

### n peque√±o (n < 10)
- La distribuci√≥n puede estar a√∫n bastante alejada de la normal
- Depende mucho de la distribuci√≥n original

### n moderado (10 ‚â§ n < 30)
- Razonable aproximaci√≥n si la distribuci√≥n original no es muy asim√©trica
- Suficiente para muchas aplicaciones pr√°cticas

### n grande (n ‚â• 30)
- Regla general: buena aproximaci√≥n para la mayor√≠a de distribuciones
- Excelente aproximaci√≥n para n > 100

### n muy grande (n > 1000)
- Aproximaci√≥n excelente pr√°cticamente siempre
- Errores de aproximaci√≥n despreciables

## Aplicaciones

### Ciencia de Datos
- **Bootstrap**: Remuestreo para estimar distribuciones de estad√≠sticos
- **Machine Learning**: Inicializaci√≥n de pesos en redes neuronales
- **Feature Engineering**: Normalizaci√≥n de caracter√≠sticas

### Estad√≠stica Inferencial
- **Intervalos de Confianza**: Construcci√≥n de intervalos para medias
- **Pruebas de Hip√≥tesis**: Test t, test z para medias
- **Tama√±o de Muestra**: Calcular n necesario para precisi√≥n deseada

### Control de Calidad
- **Gr√°ficos de Control**: L√≠mites basados en distribuci√≥n normal
- **Six Sigma**: Reducci√≥n de variabilidad en procesos
- **Muestreo de Aceptaci√≥n**: Decisiones sobre lotes

### Finanzas
- **Teor√≠a de Carteras**: Distribuci√≥n de retornos de portafolio
- **Modelo Black-Scholes**: Valoraci√≥n de opciones
- **Value at Risk (VaR)**: Estimaci√≥n de riesgo

### Telecomunicaciones
- **Teor√≠a de Tr√°fico**: Modelado de carga en redes
- **Procesamiento de Se√±ales**: Ruido aproximado como normal

## Error Est√°ndar

El **error est√°ndar** de la media es la desviaci√≥n est√°ndar del promedio muestral:

```
SE = œÉ / ‚àön
```

**Interpretaci√≥n:** A medida que n aumenta, el error est√°ndar disminuye (m√°s precisi√≥n)

**Ejemplo:**
- œÉ = 10, n = 25: SE = 10/5 = 2
- œÉ = 10, n = 100: SE = 10/10 = 1
- œÉ = 10, n = 400: SE = 10/20 = 0.5

Para reducir el SE a la mitad, necesitamos 4 veces m√°s datos.

## Condiciones de Aplicabilidad

### Cu√°ndo NO usar el TLC:

1. **Muestras muy peque√±as** (n < 10) con distribuci√≥n original muy asim√©trica
2. **Dependencia entre observaciones** (series de tiempo, datos correlacionados)
3. **Distribuciones con varianza infinita** (Cauchy, algunas Pareto)
4. **Outliers extremos** que dominan la suma
5. **Muestreo sin reemplazo** de poblaci√≥n peque√±a (usar correcci√≥n finita)

### Correcciones y Alternativas:

- **Bootstrap**: M√©todo no param√©trico para estimar distribuciones
- **Test t de Student**: Cuando œÉ es desconocida y n < 30
- **Transformaciones**: Log, ra√≠z cuadrada para reducir asimetr√≠a
- **Test no param√©tricos**: Cuando el TLC no aplica

## Demostraci√≥n Visual

```
Distribuci√≥n Original (puede ser cualquiera):
‚ñÅ‚ñÉ‚ñÖ‚ñá‚ñà‚ñá‚ñÖ‚ñÉ‚ñÅ

Promedios de n=5 muestras:
  ‚ñÇ‚ñÑ‚ñà‚ñÜ‚ñÇ

Promedios de n=30 muestras:
    ‚ñÉ‚ñÜ‚ñà‚ñÜ‚ñÉ

Promedios de n=100 muestras:
     ‚ñÇ‚ñà‚ñÇ     ‚Üê Campana de Gauss perfecta
```

## Generalizaci√≥n: TLC Multivariado

Para vectores aleatorios X‚Éó = (X‚ÇÅ, X‚ÇÇ, ..., X‚Çñ):

```
‚àön (XÃÑ‚Éó - Œº‚Éó) ‚Üí Normal Multivariada(0, Œ£)
```

Donde Œ£ es la matriz de covarianza.

**Aplicaci√≥n:** Inferencia sobre m√∫ltiples par√°metros simult√°neamente

## Juegos Relacionados

üéÆ [Lanza la Moneda](/game/coin-flip) - Observa el TLC en acci√≥n con lanzamientos repetidos

üéÆ [Constructor de Distribuciones](/game/distribution-builder) - Experimenta con diferentes distribuciones y promedios

üéÆ [Adivina la Medida](/game/guess-measure) - Practica con medias muestrales

## Recursos Adicionales

- Simuladores interactivos del TLC online
- Ley de los Grandes N√∫meros: Teorema relacionado
- Desigualdad de Chebyshev: Cota sin asumir normalidad
- Distribuci√≥n t de Student: Para muestras peque√±as
- Teorema de Berry-Esseen: Cuantifica la velocidad de convergencia
- Video: "Central Limit Theorem" - Khan Academy

---

*Volver a: [¬øQu√© es la Probabilidad?](/wiki/que-es-probabilidad)*
