# Regresi√≥n Lineal en Machine Learning

## Introducci√≥n

La **regresi√≥n lineal** es uno de los algoritmos fundamentales en Machine Learning para predecir valores continuos. A pesar de su simplicidad, sigue siendo ampliamente utilizada debido a su interpretabilidad, eficiencia computacional y efectividad en muchos problemas del mundo real. Es el punto de partida ideal para entender algoritmos de ML m√°s complejos.

## Conceptos Fundamentales

### ¬øQu√© es Regresi√≥n Lineal?

Modelar la relaci√≥n entre una o m√°s variables independientes (X) y una variable dependiente continua (Y) mediante una funci√≥n lineal.

**Ecuaci√≥n Simple:**
```
y = Œ≤‚ÇÄ + Œ≤‚ÇÅx + Œµ
```

Donde:
- **y:** Variable dependiente (target)
- **x:** Variable independiente (feature)
- **Œ≤‚ÇÄ:** Intercepto (bias)
- **Œ≤‚ÇÅ:** Pendiente (peso)
- **Œµ:** Error aleatorio

**Regresi√≥n M√∫ltiple:**
```
y = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ + ... + Œ≤‚Çôx‚Çô + Œµ
```

### Objetivo

Encontrar los valores de Œ≤ que minimizan el error entre predicciones y valores reales.

## Funci√≥n de Costo

### Mean Squared Error (MSE)

```
MSE = (1/n) Œ£(yi - ≈∑i)¬≤
```

Donde:
- **yi:** Valor real
- **≈∑i:** Valor predicho
- **n:** N√∫mero de observaciones

**Interpretaci√≥n:** Promedio de los errores al cuadrado.

### Root Mean Squared Error (RMSE)

```
RMSE = ‚àöMSE
```

**Ventaja:** Mismas unidades que la variable objetivo.

### Mean Absolute Error (MAE)

```
MAE = (1/n) Œ£|yi - ≈∑i|
```

**Ventaja:** Menos sensible a outliers que MSE.

## M√©todos de Soluci√≥n

### 1. Ecuaci√≥n Normal (Closed-Form)

Soluci√≥n anal√≠tica directa:

```
Œ≤ = (X·µÄX)‚Åª¬πX·µÄy
```

**Ventajas:**
- Soluci√≥n exacta
- No requiere iteraciones
- No hay hiperpar√°metros

**Desventajas:**
- Costoso computacionalmente para n grande
- Requiere invertir matriz (puede ser inestable)
- No escalable

### 2. Gradient Descent

M√©todo iterativo de optimizaci√≥n:

```
Œ≤ := Œ≤ - Œ± ‚àáJ(Œ≤)
```

Donde:
- **Œ±:** Learning rate (tasa de aprendizaje)
- **‚àáJ(Œ≤):** Gradiente de la funci√≥n de costo

**Proceso:**
1. Inicializar Œ≤ aleatoriamente
2. Calcular predicciones
3. Calcular gradiente
4. Actualizar Œ≤
5. Repetir hasta convergencia

**Ventajas:**
- Escalable a grandes datasets
- Funciona cuando matriz no es invertible
- Base para algoritmos m√°s complejos

**Variantes:**
- **Batch:** Usa todos los datos
- **Stochastic (SGD):** Un ejemplo a la vez
- **Mini-Batch:** Lotes peque√±os

## Supuestos de Regresi√≥n Lineal

### 1. Linealidad

La relaci√≥n entre X y Y es lineal.

**Verificaci√≥n:** Scatter plot de X vs Y
**Soluci√≥n:** Transformaciones (log, polinomial)

### 2. Independencia

Las observaciones son independientes.

**Verificaci√≥n:** Durbin-Watson test
**Problema:** Datos de series temporales

### 3. Homocedasticidad

Varianza constante de errores.

**Verificaci√≥n:** Plot residuos vs predichos
**Soluci√≥n:** Transformaciones, Weighted Least Squares

### 4. Normalidad de Errores

Los residuos siguen distribuci√≥n normal.

**Verificaci√≥n:** Q-Q plot, test de normalidad
**Importancia:** Cr√≠tico para intervalos de confianza

### 5. No Multicolinealidad

Features no est√°n altamente correlacionadas.

**Verificaci√≥n:** VIF (Variance Inflation Factor)
**Soluci√≥n:** Eliminar features, PCA, regularizaci√≥n

## M√©tricas de Evaluaci√≥n

### R¬≤ (Coeficiente de Determinaci√≥n)

```
R¬≤ = 1 - (SSR/SST)
```

Donde:
- **SSR:** Suma de cuadrados de residuos
- **SST:** Suma total de cuadrados

**Interpretaci√≥n:**
- R¬≤ = 0.8 ‚Üí 80% de varianza explicada
- R¬≤ = 0 ‚Üí Modelo no mejor que la media
- R¬≤ < 0 ‚Üí Modelo peor que la media

**Limitaci√≥n:** Siempre aumenta al agregar features.

### R¬≤ Ajustado

Penaliza por n√∫mero de features:

```
R¬≤_adj = 1 - [(1-R¬≤)(n-1)/(n-p-1)]
```

Donde:
- **n:** N√∫mero de observaciones
- **p:** N√∫mero de features

### Gr√°ficos de Diagn√≥stico

**1. Residuos vs Predichos:**
- Patr√≥n aleatorio ‚Üí Buen ajuste
- Patr√≥n sistem√°tico ‚Üí Problemas

**2. Q-Q Plot:**
- Puntos en l√≠nea ‚Üí Normalidad
- Desviaciones ‚Üí Violaci√≥n de supuesto

**3. Residuos vs Leverage:**
- Identificar puntos influyentes

**4. Scale-Location:**
- Verificar homocedasticidad

## Regularizaci√≥n

T√©cnicas para prevenir overfitting.

### Ridge Regression (L2)

A√±ade penalizaci√≥n a la magnitud de Œ≤:

```
J(Œ≤) = MSE + Œ± Œ£Œ≤i¬≤
```

**Efecto:**
- Reduce magnitud de coeficientes
- No elimina features (Œ≤ ‚âà 0 pero no = 0)
- √ötil con multicolinealidad

### Lasso Regression (L1)

Penalizaci√≥n con valor absoluto:

```
J(Œ≤) = MSE + Œ± Œ£|Œ≤i|
```

**Efecto:**
- Puede poner coeficientes exactamente a 0
- Feature selection autom√°tico
- Produce modelos sparse

### Elastic Net

Combina L1 y L2:

```
J(Œ≤) = MSE + Œ±‚ÇÅ Œ£|Œ≤i| + Œ±‚ÇÇ Œ£Œ≤i¬≤
```

**Ventaja:** Balance entre Ridge y Lasso.

## Ejemplo Pr√°ctico

### Predicci√≥n de Precios de Casas

**Dataset:** Boston Housing

**Features:**
- CRIM: Tasa de criminalidad
- RM: N√∫mero promedio de habitaciones
- DIS: Distancia a centros de empleo
- LSTAT: % poblaci√≥n de bajos ingresos

**Target:** MEDV (Precio mediano en $1000s)

**Proceso:**
```python
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Dividir datos
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Entrenar modelo
modelo = LinearRegression()
modelo.fit(X_train, y_train)

# Predecir
y_pred = modelo.predict(X_test)

# Evaluar
rmse = mean_squared_error(y_test, y_pred, squared=False)
r2 = r2_score(y_test, y_pred)

print(f"RMSE: {rmse:.2f}")
print(f"R¬≤: {r2:.3f}")
```

**Resultados:**
- RMSE: $4,500
- R¬≤: 0.72
- Coeficiente RM: +5.2 (m√°s habitaciones ‚Üí mayor precio)
- Coeficiente LSTAT: -3.8 (m√°s pobreza ‚Üí menor precio)

## Extensiones de Regresi√≥n Lineal

### Regresi√≥n Polinomial

A√±adir t√©rminos de mayor grado:

```
y = Œ≤‚ÇÄ + Œ≤‚ÇÅx + Œ≤‚ÇÇx¬≤ + Œ≤‚ÇÉx¬≥ + ...
```

**Uso:** Capturar relaciones no lineales
**Cuidado:** Overfitting con grados altos

### Interacciones

Incluir productos de features:

```
y = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ + Œ≤‚ÇÉ(x‚ÇÅ¬∑x‚ÇÇ)
```

**Ejemplo:** Efecto de publicidad depende de precio

### Regresi√≥n Log√≠stica

Para clasificaci√≥n binaria (extensi√≥n de regresi√≥n lineal con funci√≥n sigmoide).

## Ventajas de Regresi√≥n Lineal

‚úÖ **Simplicidad:** F√°cil de entender e implementar
‚úÖ **Interpretabilidad:** Coeficientes tienen significado claro
‚úÖ **Eficiencia:** R√°pido de entrenar y predecir
‚úÖ **Baseline:** Excelente punto de partida
‚úÖ **Estad√≠stica:** Intervalos de confianza, p-values

## Limitaciones

‚ùå **Supuestos restrictivos:** Linealidad, normalidad, etc.
‚ùå **No captura no-linealidad:** Sin transformaciones
‚ùå **Sensible a outliers:** Especialmente con MSE
‚ùå **Multicolinealidad:** Coeficientes inestables
‚ùå **Overfitting:** Con muchas features

## Cu√°ndo Usar Regresi√≥n Lineal

‚úÖ **Relaci√≥n aproximadamente lineal**
‚úÖ **Interpretabilidad es importante**
‚úÖ **Baseline r√°pido**
‚úÖ **Dataset peque√±o a mediano**
‚úÖ **Features no correlacionadas**

## Alternativas

**Cuando regresi√≥n lineal no es suficiente:**

- **√Årboles de Decisi√≥n:** No linealidad, no supuestos
- **Random Forest:** M√∫ltiples √°rboles, robusto
- **Gradient Boosting:** XGBoost, LightGBM, alta precisi√≥n
- **Redes Neuronales:** Patrones complejos
- **SVR:** Support Vector Regression para no linealidad
- **GAM:** Generalized Additive Models

## Implementaci√≥n en Diferentes Lenguajes

### Python (Scikit-learn)
```python
from sklearn.linear_model import LinearRegression, Ridge, Lasso

# Lineal simple
modelo = LinearRegression()
modelo.fit(X_train, y_train)

# Ridge
ridge = Ridge(alpha=1.0)
ridge.fit(X_train, y_train)

# Lasso
lasso = Lasso(alpha=0.1)
lasso.fit(X_train, y_train)
```

### R
```R
# Regresi√≥n simple
modelo <- lm(y ~ x1 + x2 + x3, data=df)
summary(modelo)

# Ridge
library(glmnet)
ridge <- glmnet(X, y, alpha=0)

# Lasso
lasso <- glmnet(X, y, alpha=1)
```

## Mejores Pr√°cticas

1. **Explorar datos:** Visualizar relaciones
2. **Feature engineering:** Transformaciones, interacciones
3. **Escalar features:** Especialmente para regularizaci√≥n
4. **Validar supuestos:** Gr√°ficos de diagn√≥stico
5. **Cross-validation:** Evaluar generalizaci√≥n
6. **Regularizaci√≥n:** Prevenir overfitting
7. **Interpretar resultados:** No solo m√©tricas

## Juegos Relacionados

üéÆ [Regresi√≥n Lineal Builder](/game/linear-regression) - Ajusta l√≠neas de regresi√≥n interactivamente

üéÆ [Detector de Correlaci√≥n](/game/correlation-detector) - Visualiza relaciones lineales

## Recursos Adicionales

- Statsmodels (Python): Inferencia estad√≠stica detallada
- Ordinary Least Squares (OLS): Fundamentos matem√°ticos
- Generalized Linear Models (GLM): Extensi√≥n para otras distribuciones
- Bayesian Linear Regression: Enfoque probabil√≠stico
- Online Learning: Actualizaci√≥n incremental

---

*Anterior: [Tipos de Aprendizaje](/wiki/tipos-aprendizaje) | Siguiente: [√Årboles de Decisi√≥n](/wiki/arboles-decision)*
