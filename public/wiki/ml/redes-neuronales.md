# Redes Neuronales

## Introducci√≥n

Las **redes neuronales** son modelos de Machine Learning inspirados en el cerebro humano, compuestos por capas de neuronas artificiales interconectadas. Son la base del Deep Learning y han revolucionado campos como visi√≥n por computadora, procesamiento de lenguaje natural y generaci√≥n de contenido. Su capacidad para aprender representaciones jer√°rquicas complejas las hace extremadamente poderosas.

## Conceptos Fundamentales

### Neurona Artificial (Perceptr√≥n)

Unidad b√°sica que:
1. Recibe inputs (x‚ÇÅ, x‚ÇÇ, ..., x‚Çô)
2. Multiplica por pesos (w‚ÇÅ, w‚ÇÇ, ..., w‚Çô)
3. Suma: z = w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + ... + b (bias)
4. Aplica funci√≥n de activaci√≥n: a = f(z)

### Arquitectura de Red

**Componentes:**
- **Capa de Entrada:** Recibe datos
- **Capas Ocultas:** Procesan informaci√≥n
- **Capa de Salida:** Genera predicci√≥n

**Tipos:**
- **Fully Connected (Dense):** Cada neurona conectada a todas las anteriores
- **Convolutional (CNN):** Para im√°genes
- **Recurrent (RNN/LSTM):** Para secuencias
- **Transformer:** Para NLP moderno

## Funciones de Activaci√≥n

### Sigmoid
```
œÉ(x) = 1 / (1 + e^(-x))
```
Salida: (0, 1)
**Uso:** Clasificaci√≥n binaria en salida

### Tanh
```
tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))
```
Salida: (-1, 1)
**Ventaja:** Centrada en 0

### ReLU (Rectified Linear Unit)
```
ReLU(x) = max(0, x)
```
**M√°s usado:** R√°pido, evita vanishing gradient

### Leaky ReLU
```
LeakyReLU(x) = max(Œ±x, x)
```
**Ventaja:** No "muere" con valores negativos

### Softmax
```
softmax(xi) = e^xi / Œ£e^xj
```
**Uso:** Clasificaci√≥n multiclase en salida

## Proceso de Entrenamiento

### Forward Propagation

1. Input pasa por capas
2. Cada neurona calcula: a = f(Wx + b)
3. Output final en capa de salida

### Loss Function

**Regresi√≥n:**
- Mean Squared Error (MSE)
- Mean Absolute Error (MAE)

**Clasificaci√≥n:**
- Binary Cross-Entropy (binaria)
- Categorical Cross-Entropy (multiclase)

### Backpropagation

1. Calcular error en salida
2. Propagar error hacia atr√°s
3. Calcular gradientes: ‚àÇL/‚àÇw, ‚àÇL/‚àÇb
4. Actualizar pesos: w := w - Œ±‚àáL

### Optimizadores

**SGD (Stochastic Gradient Descent):**
```
w := w - Œ±‚àáL
```

**Momentum:**
```
v := Œ≤v + ‚àáL
w := w - Œ±v
```

**Adam (Adaptive Moment Estimation):**
Combina momentum y tasa de aprendizaje adaptativa.
**M√°s usado actualmente.**

**RMSprop:**
Ajusta learning rate por par√°metro.

## Regularizaci√≥n

### Dropout

Durante entrenamiento, "apagar" aleatoriamente neuronas.
**Tasa t√≠pica:** 0.2-0.5

**Ventaja:** Previene co-adaptaci√≥n de neuronas.

### L1/L2 Regularization

Penalizar pesos grandes:
```
Loss_total = Loss + Œª Œ£|wi| (L1)
Loss_total = Loss + Œª Œ£wi¬≤ (L2)
```

### Batch Normalization

Normalizar activaciones entre capas.

**Beneficios:**
- Entrenamiento m√°s r√°pido
- Permite learning rates mayores
- Reduce dependencia de inicializaci√≥n

### Early Stopping

Detener entrenamiento cuando validaci√≥n deja de mejorar.

## Hiperpar√°metros

**Arquitectura:**
- N√∫mero de capas
- Neuronas por capa
- Funci√≥n de activaci√≥n

**Entrenamiento:**
- Learning rate (Œ±)
- Batch size
- Epochs
- Optimizer

**Regularizaci√≥n:**
- Dropout rate
- L1/L2 lambda
- Batch normalization

## Tipos de Redes Neuronales

### Feedforward Neural Networks (FNN)

Red b√°sica, informaci√≥n fluye solo hacia adelante.

**Uso:**
- Clasificaci√≥n tabular
- Regresi√≥n
- Baseline

### Convolutional Neural Networks (CNN)

Especializadas en datos con estructura espacial.

**Componentes:**
- **Capas convolucionales:** Detectan patrones locales
- **Pooling:** Reduce dimensionalidad
- **Fully connected:** Clasificaci√≥n final

**Aplicaciones:**
- Reconocimiento de im√°genes
- Detecci√≥n de objetos
- Segmentaci√≥n
- Video processing

**Arquitecturas famosas:**
- LeNet, AlexNet, VGG
- ResNet, Inception
- EfficientNet, Vision Transformer

### Recurrent Neural Networks (RNN)

Para secuencias, mantienen "memoria" de inputs anteriores.

**Problemas:**
- Vanishing gradient en secuencias largas

**Soluci√≥n:** LSTM, GRU

**LSTM (Long Short-Term Memory):**
- Gates: Forget, Input, Output
- Mantiene informaci√≥n relevante largo plazo

**Aplicaciones:**
- Series temporales
- Procesamiento de texto
- Reconocimiento de voz
- Generaci√≥n de m√∫sica

### Transformers

Arquitectura moderna basada en atenci√≥n.

**Componentes:**
- **Self-Attention:** Relaciona elementos de secuencia
- **Multi-Head Attention:** M√∫ltiples representaciones
- **Positional Encoding:** Informaci√≥n de posici√≥n

**Ventajas:**
- Paralelizable (vs RNN secuencial)
- Captura dependencias largas
- State-of-the-art en NLP

**Modelos famosos:**
- BERT, GPT, T5
- Vision Transformer (ViT)

### Autoencoders

Aprender representaciones comprimidas de datos.

**Arquitectura:**
- Encoder: Comprimir
- Decoder: Reconstruir

**Aplicaciones:**
- Reducci√≥n de dimensionalidad
- Detecci√≥n de anomal√≠as
- Generaci√≥n de im√°genes
- Denoising

### GANs (Generative Adversarial Networks)

Dos redes compitiendo:
- **Generator:** Crea datos falsos
- **Discriminator:** Distingue real vs falso

**Aplicaciones:**
- Generaci√≥n de im√°genes realistas
- Style transfer
- Data augmentation
- Super-resolution

## Ventajas de Redes Neuronales

‚úÖ **Universal approximator:** Puede aproximar cualquier funci√≥n
‚úÖ **Feature learning autom√°tico:** No requiere feature engineering manual
‚úÖ **Flexibilidad:** Arquitecturas para diferentes tipos de datos
‚úÖ **Estado del arte:** Mejores resultados en muchos problemas
‚úÖ **Transfer learning:** Reusar modelos pre-entrenados

## Desventajas

‚ùå **Requiere muchos datos:** Miles a millones de ejemplos
‚ùå **Computacionalmente costoso:** GPUs necesarias
‚ùå **Caja negra:** Dif√≠cil de interpretar
‚ùå **Hiperpar√°metros:** Muchos para tunear
‚ùå **Overfitting:** Sin regularizaci√≥n adecuada
‚ùå **Tiempo de entrenamiento:** Horas a d√≠as

## Frameworks y Librer√≠as

### TensorFlow/Keras
```python
from tensorflow import keras

model = keras.Sequential([
    keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(64, activation='relu'),
    keras.layers.Dense(10, activation='softmax')
])

model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

model.fit(X_train, y_train, epochs=10, batch_size=32)
```

### PyTorch
```python
import torch.nn as nn

class NeuralNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer1 = nn.Linear(784, 128)
        self.layer2 = nn.Linear(128, 64)
        self.layer3 = nn.Linear(64, 10)
        self.relu = nn.ReLU()
        
    def forward(self, x):
        x = self.relu(self.layer1(x))
        x = self.relu(self.layer2(x))
        return self.layer3(x)
```

## Mejores Pr√°cticas

1. **Normalizar inputs:** Mean=0, Std=1
2. **Inicializaci√≥n:** Xavier/He initialization
3. **Batch size:** 32-256 t√≠picamente
4. **Learning rate:** Empezar con 0.001 (Adam)
5. **Monitorear:** Validation loss/accuracy
6. **Regularizaci√≥n:** Dropout + L2
7. **Data augmentation:** Especialmente en im√°genes
8. **Transfer learning:** Usar modelos pre-entrenados cuando sea posible

## Aplicaciones del Mundo Real

### Visi√≥n por Computadora
- Reconocimiento facial
- Conducci√≥n aut√≥noma
- Diagn√≥stico m√©dico por im√°genes
- Control de calidad industrial

### Procesamiento de Lenguaje Natural
- Traducci√≥n autom√°tica
- Chatbots y asistentes virtuales
- An√°lisis de sentimientos
- Resumen de textos

### Generaci√≥n de Contenido
- DALL-E, Stable Diffusion (im√°genes)
- GPT (texto)
- MuseNet (m√∫sica)
- DeepFake (video)

### Juegos y Simulaci√≥n
- AlphaGo, AlphaZero
- OpenAI Five (Dota 2)
- Simuladores de f√≠sica

### Otros
- Recomendaci√≥n (Netflix, Spotify)
- Predicci√≥n de series temporales
- Detecci√≥n de fraude
- Drug discovery

## Juegos Relacionados

üéÆ [Clasificador Visual](/game/visual-classifier) - Visualiza c√≥mo aprenden modelos

üéÆ [Overfitting Game](/game/overfitting-game) - Experimenta con complejidad de redes

## Recursos Adicionales

- deeplearning.ai: Cursos de Andrew Ng
- Fast.ai: Practical Deep Learning
- Neural Networks and Deep Learning (Michael Nielsen)
- PyTorch Tutorials
- TensorFlow Tutorials
- Papers with Code: State-of-the-art

---

*Anterior: [Random Forest](/wiki/random-forest) | Siguiente: [Evaluaci√≥n de Modelos](/wiki/evaluacion-modelos)*
