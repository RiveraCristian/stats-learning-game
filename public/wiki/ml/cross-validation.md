# Cross-Validation (Validaci√≥n Cruzada)

## Introducci√≥n

**Cross-validation** es una t√©cnica estad√≠stica para evaluar y comparar modelos de Machine Learning dividiendo los datos en m√∫ltiples subconjuntos. En lugar de una simple divisi√≥n train-test, cross-validation usa diferentes particiones para obtener una estimaci√≥n m√°s robusta y menos sesgada del desempe√±o del modelo. Es esencial para evitar overfitting y seleccionar modelos √≥ptimos.

## ¬øPor Qu√© Cross-Validation?

### Problemas con Simple Train-Test Split

‚ùå **Dependiente del split espec√≠fico:** Resultados var√≠an seg√∫n c√≥mo se dividen los datos
‚ùå **Desperdicio de datos:** Test set no se usa para entrenamiento
‚ùå **Varianza alta:** Especialmente con datasets peque√±os
‚ùå **Puede ser enga√±oso:** Un split "afortunado" da resultados optimistas

### Beneficios de Cross-Validation

‚úÖ **Estimaci√≥n m√°s robusta:** Promedia m√∫ltiples evaluaciones
‚úÖ **Uso eficiente de datos:** Todos los datos se usan para entrenar y evaluar
‚úÖ **Detecta overfitting:** Identifica modelos que no generalizan
‚úÖ **Selecci√≥n de modelos:** Compara modelos objetivamente
‚úÖ **Tuning de hiperpar√°metros:** Encuentra configuraci√≥n √≥ptima

## K-Fold Cross-Validation

### Proceso

1. **Dividir datos en K folds (particiones) iguales**
2. **Para cada fold:**
   - Usar ese fold como validaci√≥n
   - Entrenar con los K-1 folds restantes
   - Evaluar en fold de validaci√≥n
3. **Calcular m√©trica promedio** de las K iteraciones

### Ejemplo con K=5

```
Fold 1: [Test][Train][Train][Train][Train]
Fold 2: [Train][Test][Train][Train][Train]
Fold 3: [Train][Train][Test][Train][Train]
Fold 4: [Train][Train][Train][Test][Train]
Fold 5: [Train][Train][Train][Train][Test]
```

Promedio de 5 evaluaciones = estimaci√≥n final

### Elecci√≥n de K

**K=5:**
- Balance entre bias y variance
- Computacionalmente eficiente
- Recomendado para la mayor√≠a de casos

**K=10:**
- Est√°ndar en literatura
- M√°s tiempo de c√≥mputo
- Menor varianza

**K grande (‚âàn):**
- Estimaci√≥n menos sesgada
- Mayor varianza
- Muy costoso computacionalmente

## Variantes de Cross-Validation

### Stratified K-Fold

Mantiene la proporci√≥n de clases en cada fold.

**Cu√°ndo usar:**
- Clasificaci√≥n con clases desbalanceadas
- Asegurar representatividad en cada fold

**Ejemplo:**
Si dataset tiene 80% clase A y 20% clase B, cada fold tendr√° la misma proporci√≥n.

```python
from sklearn.model_selection import StratifiedKFold

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
for train_idx, val_idx in skf.split(X, y):
    X_train, X_val = X[train_idx], X[val_idx]
    y_train, y_val = y[train_idx], y[val_idx]
```

### Leave-One-Out Cross-Validation (LOOCV)

K = n (n√∫mero total de observaciones)

**Proceso:**
- Cada observaci√≥n es un fold
- Entrenar con n-1 observaciones
- Evaluar en 1 observaci√≥n
- Repetir n veces

**Ventajas:**
‚úÖ Estimaci√≥n casi insesgada
‚úÖ Determin√≠stico (no aleatorio)
‚úÖ √ötil para datasets muy peque√±os

**Desventajas:**
‚ùå Muy costoso (n entrenamientos)
‚ùå Alta varianza en estimaci√≥n
‚ùå Puede sobreestimar performance

**Cu√°ndo usar:**
- n < 100 y se necesita m√°xima precisi√≥n
- Tiempo de entrenamiento r√°pido

### Leave-P-Out

Generalizaci√≥n de LOOCV, deja P observaciones fuera.

**Problema:** N√∫mero de combinaciones C(n,p) crece exponencialmente.

### Repeated K-Fold

Repite K-Fold m√∫ltiples veces con diferentes random splits.

```python
from sklearn.model_selection import RepeatedKFold

rkf = RepeatedKFold(n_splits=5, n_repeats=10, random_state=42)
```

**Ventaja:** Reduce varianza de estimaci√≥n.

### Time Series Cross-Validation

Para datos temporales, respeta el orden cronol√≥gico.

**M√©todos:**

**1. Rolling Window (Sliding):**
```
Train: [1 2 3 4 5] Test: [6]
Train: [2 3 4 5 6] Test: [7]
Train: [3 4 5 6 7] Test: [8]
```

**2. Expanding Window:**
```
Train: [1 2 3 4 5] Test: [6]
Train: [1 2 3 4 5 6] Test: [7]
Train: [1 2 3 4 5 6 7] Test: [8]
```

**Importante:** NO usar K-Fold est√°ndar (mezcla pasado y futuro).

### Group K-Fold

Asegura que grupos relacionados no se dividan entre train y test.

**Ejemplo:**
- M√∫ltiples mediciones del mismo paciente
- Im√°genes del mismo objeto
- Transacciones del mismo usuario

```python
from sklearn.model_selection import GroupKFold

gkf = GroupKFold(n_splits=5)
for train_idx, val_idx in gkf.split(X, y, groups=patient_ids):
    ...
```

## Nested Cross-Validation

CV anidado para selecci√≥n de modelos E hiperpar√°metros.

**Estructura:**
- **Outer Loop:** Evaluar performance del modelo
- **Inner Loop:** Seleccionar hiperpar√°metros

**Ejemplo:**
```python
# Outer CV: 5 folds
outer_cv = KFold(n_splits=5)

# Inner CV: 3 folds para tuning
inner_cv = KFold(n_splits=3)

# Grid Search con inner CV
grid_search = GridSearchCV(model, param_grid, cv=inner_cv)

# Evaluar con outer CV
scores = []
for train_idx, test_idx in outer_cv.split(X):
    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]
    
    grid_search.fit(X_train, y_train)
    score = grid_search.score(X_test, y_test)
    scores.append(score)

final_score = np.mean(scores)
```

**Ventaja:** Estimaci√≥n no sesgada del performance real.

## Hold-Out vs Cross-Validation

### Hold-Out (Train-Test Split)

**Pros:**
- R√°pido
- Simple
- Suficiente para datasets grandes

**Cons:**
- Alta varianza en datasets peque√±os
- Desperdicia datos

**Cu√°ndo usar:** n > 10,000 y tiempo limitado

### Cross-Validation

**Pros:**
- Estimaci√≥n robusta
- Usa todos los datos
- Detecta variabilidad

**Cons:**
- Computacionalmente costoso
- M√°s complejo

**Cu√°ndo usar:** n < 10,000 o selecci√≥n de modelos cr√≠tica

## Aplicaciones de Cross-Validation

### 1. Evaluaci√≥n de Modelos

Estimar performance real en datos no vistos.

```python
from sklearn.model_selection import cross_val_score

scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')
print(f"Accuracy: {scores.mean():.3f} (+/- {scores.std():.3f})")
```

### 2. Selecci√≥n de Modelos

Comparar diferentes algoritmos.

```python
models = {
    'Logistic': LogisticRegression(),
    'RF': RandomForestClassifier(),
    'SVM': SVC()
}

for name, model in models.items():
    scores = cross_val_score(model, X, y, cv=5)
    print(f"{name}: {scores.mean():.3f}")
```

### 3. Tuning de Hiperpar√°metros

Con Grid Search o Random Search.

```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30]
}

grid_search = GridSearchCV(
    RandomForestClassifier(),
    param_grid,
    cv=5,
    scoring='accuracy'
)

grid_search.fit(X, y)
print(f"Best params: {grid_search.best_params_}")
print(f"Best score: {grid_search.best_score_:.3f}")
```

### 4. Feature Selection

Identificar features importantes.

```python
from sklearn.feature_selection import RFECV

selector = RFECV(estimator=model, cv=5)
selector.fit(X, y)
selected_features = X.columns[selector.support_]
```

## Consideraciones Pr√°cticas

### Computational Cost

K-Fold con K=5 entrena modelo 5 veces.
Grid Search con 10 par√°metros y CV=5 entrena 50 veces.

**Estrategias:**
- Usar submuestra para b√∫squeda inicial
- Paralelizaci√≥n (n_jobs=-1)
- Random Search en lugar de Grid Search
- Bayesian Optimization para b√∫squeda eficiente

### Random State

Fijar `random_state` para reproducibilidad:

```python
cv = KFold(n_splits=5, shuffle=True, random_state=42)
```

### Shuffle

Generalmente recomendado, excepto para series temporales:

```python
cv = KFold(n_splits=5, shuffle=True)  # Mezclar datos
```

## Errores Comunes

‚ùå **Data Leakage:** Preprocesar antes de split
```python
# INCORRECTO
scaler.fit(X)  # Usa informaci√≥n de test set
X_scaled = scaler.transform(X)
cross_val_score(model, X_scaled, y, cv=5)

# CORRECTO
cv_results = []
for train_idx, val_idx in cv.split(X):
    X_train, X_val = X[train_idx], X[val_idx]
    scaler.fit(X_train)  # Solo en train
    X_train_scaled = scaler.transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    # Entrenar y evaluar...
```

‚ùå **Usar K-Fold en Series Temporales:** Mezcla pasado y futuro

‚ùå **Overfitting to Validation:** Tunear demasiado con mismo CV

‚ùå **Ignorar Grupos:** Datos relacionados en train y test

## M√©tricas con Cross-Validation

### Clasificaci√≥n

```python
scoring = {
    'accuracy': 'accuracy',
    'precision': 'precision',
    'recall': 'recall',
    'f1': 'f1',
    'roc_auc': 'roc_auc'
}

scores = cross_validate(model, X, y, cv=5, scoring=scoring)
```

### Regresi√≥n

```python
scoring = {
    'mae': 'neg_mean_absolute_error',
    'mse': 'neg_mean_squared_error',
    'r2': 'r2'
}

scores = cross_validate(model, X, y, cv=5, scoring=scoring)
```

## Interpretaci√≥n de Resultados

### Media y Desviaci√≥n Est√°ndar

```python
scores = cross_val_score(model, X, y, cv=5)
mean_score = scores.mean()
std_score = scores.std()

print(f"Score: {mean_score:.3f} ¬± {std_score:.3f}")
```

**Interpretaci√≥n:**
- **Mean alto:** Buen desempe√±o promedio
- **Std bajo:** Performance consistente
- **Std alto:** Performance variable (posible problema)

### Intervalo de Confianza

```python
import scipy.stats as stats

confidence = 0.95
n = len(scores)
interval = stats.t.interval(
    confidence, 
    n-1, 
    loc=mean_score, 
    scale=stats.sem(scores)
)

print(f"95% CI: [{interval[0]:.3f}, {interval[1]:.3f}]")
```

## Juegos Relacionados

üéÆ [Overfitting Game](/game/overfitting-game) - Observa importancia de validaci√≥n

üéÆ [Clasificador Visual](/game/visual-classifier) - Experimenta con train-test split

## Recursos Adicionales

- Scikit-learn: Cross-validation Guide
- On Over-fitting in Model Selection (Cawley & Talbot)
- Approximately Correct Cross-Validation
- Time Series Forecasting: Cross-Validation Strategies
- Nested versus non-nested cross-validation

---

*Anterior: [Evaluaci√≥n de Modelos](/wiki/evaluacion-modelos)*
