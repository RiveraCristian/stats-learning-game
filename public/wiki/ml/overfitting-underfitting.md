# Overfitting vs Underfitting

## IntroducciÃ³n

**Overfitting** y **underfitting** son dos problemas fundamentales en machine learning que afectan la capacidad de un modelo para generalizar a nuevos datos. Encontrar el balance correcto es crucial para construir modelos efectivos.

## Â¿QuÃ© es Underfitting?

**Underfitting** ocurre cuando un modelo es demasiado simple para capturar los patrones subyacentes en los datos.

### CaracterÃ­sticas
- **Alto bias (sesgo)**
- Bajo rendimiento en datos de entrenamiento
- Bajo rendimiento en datos de prueba
- El modelo es demasiado general

### Ejemplo Visual
Imagina intentar ajustar una lÃ­nea recta a datos que siguen una curva:
```
Datos: Forma de U
Modelo: LÃ­nea recta â”€â”€â”€â”€â”€
Resultado: No captura el patrÃ³n
```

### Causas Comunes
- Modelo demasiado simple (ej: regresiÃ³n lineal para datos no lineales)
- Pocas features
- RegularizaciÃ³n excesiva
- Entrenamiento insuficiente

## Â¿QuÃ© es Overfitting?

**Overfitting** ocurre cuando un modelo aprende no solo los patrones reales sino tambiÃ©n el ruido en los datos de entrenamiento.

### CaracterÃ­sticas
- **Alta varianza**
- Excelente rendimiento en datos de entrenamiento
- Pobre rendimiento en datos de prueba
- El modelo memoriza en lugar de aprender

### Ejemplo Visual
```
Datos de entrenamiento: â€¢ â€¢ â€¢ â€¢ â€¢
Modelo overfitted: ~~~âˆ¿âˆ¿âˆ¿~~~
Modelo correcto: â”€â”€â”€â”€â”€â”€â”€â”€
```

El modelo overfitted pasa por cada punto exactamente, incluyendo el ruido.

### Causas Comunes
- Modelo demasiado complejo
- Demasiadas features
- Poco datos de entrenamiento
- Entrenamiento excesivo
- Sin regularizaciÃ³n

## Bias-Variance Tradeoff

El dilema fundamental en machine learning:

```
Error Total = BiasÂ² + Varianza + Ruido Irreducible
```

### Bias (Sesgo)
- Error por simplificaciones en el modelo
- Modelos simples â†’ Alto bias
- **Underfitting** estÃ¡ asociado con alto bias

### Varianza
- Error por sensibilidad a fluctuaciones en datos
- Modelos complejos â†’ Alta varianza
- **Overfitting** estÃ¡ asociado con alta varianza

### El Tradeoff
```
Modelo Simple â†’ Alto Bias, Baja Varianza â†’ Underfitting
Modelo Complejo â†’ Bajo Bias, Alta Varianza â†’ Overfitting
Modelo Ã“ptimo â†’ Balance entre Bias y Varianza
```

## Curvas de Aprendizaje

GrÃ¡ficos que muestran el rendimiento del modelo vs tamaÃ±o de datos o complejidad.

### Curva TÃ­pica de Underfitting
```
Error
  â†‘
  |  Train â”€â”€â”€â”€â”€â”€â”€â”€
  |  Test  â”€â”€â”€â”€â”€â”€â”€â”€
  |  (ambos altos, cercanos)
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ TamaÃ±o de datos
```

### Curva TÃ­pica de Overfitting
```
Error
  â†‘
  |  Test  â”€â”€â”€â”€â”€â”€â”€â”€
  |  Train _____ (muy bajo)
  |  (gran brecha)
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ TamaÃ±o de datos
```

### Curva Ideal
```
Error
  â†‘
  |  Test  â”€â”€â”€â”€â”€â”€
  |  Train â”€â”€â”€â”€â”€â”€ (cercanos, ambos bajos)
  |
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ TamaÃ±o de datos
```

## DetecciÃ³n

### SeÃ±ales de Underfitting
- âœ— Accuracy bajo en entrenamiento (<80%)
- âœ— Accuracy bajo en validaciÃ³n
- âœ— Poca diferencia entre train y test
- âœ— PÃ©rdida alta que no disminuye

### SeÃ±ales de Overfitting
- âœ— Accuracy muy alto en entrenamiento (>95%)
- âœ— Accuracy bajo en validaciÃ³n
- âœ— **Gran brecha** entre train y test
- âœ— PÃ©rdida de entrenamiento disminuye pero validaciÃ³n aumenta

### Modelo Bien Ajustado
- âœ“ Buen accuracy en entrenamiento (~85-90%)
- âœ“ Accuracy similar en validaciÃ³n
- âœ“ PequeÃ±a brecha entre train y test
- âœ“ Ambas pÃ©rdidas disminuyen juntas

## Soluciones para Underfitting

### 1. Aumentar Complejidad del Modelo
- Usar modelos mÃ¡s complejos (ej: de lineal a polinomial)
- Agregar mÃ¡s capas (redes neuronales)
- Aumentar nÃºmero de neuronas

### 2. Agregar Features
- Feature engineering
- Interacciones entre variables
- Transformaciones no lineales

### 3. Reducir RegularizaciÃ³n
- Disminuir Î» en Ridge/Lasso
- Reducir dropout
- Menos restricciones

### 4. Entrenar MÃ¡s Tiempo
- MÃ¡s Ã©pocas
- Mejor optimizaciÃ³n

## Soluciones para Overfitting

### 1. MÃ¡s Datos de Entrenamiento
La soluciÃ³n mÃ¡s efectiva:
- Recolectar mÃ¡s datos
- Data augmentation (imÃ¡genes, texto)
- Synthetic data generation

### 2. RegularizaciÃ³n

**L1 (Lasso)**:
```
Loss = MSE + Î»Î£|w|
```
- Fuerza algunos pesos a cero
- Feature selection automÃ¡tica

**L2 (Ridge)**:
```
Loss = MSE + Î»Î£wÂ²
```
- Penaliza pesos grandes
- Mantiene todas las features

**Elastic Net**:
Combina L1 y L2

### 3. Dropout (Redes Neuronales)
- Desactiva aleatoriamente neuronas durante entrenamiento
- TÃ­picamente 20-50%
- Previene co-adaptaciÃ³n de neuronas

### 4. Early Stopping
- Monitorear pÃ©rdida de validaciÃ³n
- Detener cuando empieza a aumentar
- Guardar el mejor modelo

### 5. Reducir Complejidad
- Menos features (feature selection)
- Modelos mÃ¡s simples
- Menos capas/neuronas

### 6. Cross-Validation
- K-fold cross-validation
- Evaluar generalizaciÃ³n
- Detectar overfitting temprano

### 7. Ensemble Methods
- Bagging (Random Forest)
- Boosting (XGBoost, AdaBoost)
- Stacking

## Ejemplo PrÃ¡ctico: RegresiÃ³n Polinomial

### Datos
RelaciÃ³n cuadrÃ¡tica: y = xÂ² + ruido

### Modelos

**Underfitting** (grado 1):
```python
y = Î²â‚€ + Î²â‚x
# RÂ² train = 0.65, RÂ² test = 0.63
```

**Bien ajustado** (grado 2):
```python
y = Î²â‚€ + Î²â‚x + Î²â‚‚xÂ²
# RÂ² train = 0.95, RÂ² test = 0.93
```

**Overfitting** (grado 10):
```python
y = Î²â‚€ + Î²â‚x + Î²â‚‚xÂ² + ... + Î²â‚â‚€xÂ¹â°
# RÂ² train = 0.99, RÂ² test = 0.45
```

## ValidaciÃ³n Cruzada

TÃ©cnica esencial para detectar overfitting:

### K-Fold Cross-Validation
1. Dividir datos en K partes
2. Entrenar en K-1 partes
3. Validar en la parte restante
4. Repetir K veces
5. Promediar resultados

**Ventaja**: Usa todos los datos para entrenamiento y validaciÃ³n.

## RegularizaciÃ³n en PrÃ¡ctica

### Ejemplo con Ridge Regression

```python
# Sin regularizaciÃ³n (Î» = 0)
# Puede overfittear

# RegularizaciÃ³n moderada (Î» = 1)
# Balance Ã³ptimo

# RegularizaciÃ³n excesiva (Î» = 1000)
# Underfitting
```

### Seleccionar Î»
- Grid search
- Cross-validation
- Curva de validaciÃ³n

## Complejidad del Modelo

### Navaja de Occam
"Entre modelos con rendimiento similar, prefiere el mÃ¡s simple"

**Razones**:
- MÃ¡s interpretable
- MÃ¡s rÃ¡pido
- Menos propenso a overfitting
- MÃ¡s robusto

## MÃ©tricas de EvaluaciÃ³n

### Para ClasificaciÃ³n
- **Accuracy**: Correctos / Total
- **Precision**: VP / (VP + FP)
- **Recall**: VP / (VP + FN)
- **F1-Score**: Media armÃ³nica de precision y recall

### Para RegresiÃ³n
- **MSE**: Mean Squared Error
- **RMSE**: Root MSE
- **MAE**: Mean Absolute Error
- **RÂ²**: Coeficiente de determinaciÃ³n

## Estrategias Generales

### Pipeline Recomendado
1. **Baseline simple** (detectar underfitting)
2. **Aumentar complejidad gradualmente**
3. **Monitorear train vs test**
4. **Aplicar regularizaciÃ³n si es necesario**
5. **Validar con cross-validation**
6. **Evaluar en test set final**

### Reglas PrÃ¡cticas
- Train accuracy > 90%, Test << Train â†’ **Overfitting**
- Train accuracy < 80%, Test â‰ˆ Train â†’ **Underfitting**
- Train â‰ˆ Test, ambos ~85-90% â†’ **Bien ajustado**

## ConclusiÃ³n

**Balance es la clave**:
- âš–ï¸ Ni muy simple (underfitting)
- âš–ï¸ Ni muy complejo (overfitting)
- âš–ï¸ Justo en el punto Ã³ptimo

**Recuerda**:
- MÃ¡s datos casi siempre ayuda
- RegularizaciÃ³n es tu amiga
- Siempre valida en datos no vistos
- La simplicidad tiene valor

## Juegos Relacionados

ðŸŽ® [Overfitting Game](/game/overfitting-game) - Encuentra el balance perfecto entre underfitting y overfitting

---

*Siguiente: [EvaluaciÃ³n de Modelos](/wiki/evaluacion-modelos)*
