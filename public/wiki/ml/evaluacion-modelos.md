# Evaluaci√≥n de Modelos de Machine Learning

## Introducci√≥n

La **evaluaci√≥n de modelos** es fundamental para determinar si un modelo de Machine Learning es efectivo y generaliza bien a datos nuevos. Una evaluaci√≥n apropiada va m√°s all√° de la simple precisi√≥n, considerando m√∫ltiples m√©tricas, validaci√≥n cruzada y an√°lisis de errores. Elegir las m√©tricas correctas es cr√≠tico para el √©xito del proyecto.

## M√©tricas de Clasificaci√≥n

### Matriz de Confusi√≥n

Base para muchas m√©tricas:

```
                Predicho Positivo  Predicho Negativo
Real Positivo       TP                  FN
Real Negativo       FP                  TN
```

- **TP (True Positive):** Correctamente clasificados como positivos
- **TN (True Negative):** Correctamente clasificados como negativos
- **FP (False Positive):** Error Tipo I
- **FN (False Negative):** Error Tipo II

### Accuracy (Exactitud)

```
Accuracy = (TP + TN) / (TP + TN + FP + FN)
```

**Cu√°ndo usar:** Clases balanceadas

**Limitaci√≥n:** Enga√±osa con clases desbalanceadas

**Ejemplo:** 95% negativos ‚Üí predecir siempre negativo da 95% accuracy

### Precision (Precisi√≥n)

```
Precision = TP / (TP + FP)
```

**Interpretaci√≥n:** De los predichos como positivos, ¬øcu√°ntos son realmente positivos?

**Uso:** Cuando false positives son costosos

**Ejemplo:** Detecci√≥n de spam (no quieres marcar emails leg√≠timos)

### Recall (Sensibilidad/Sensitivity)

```
Recall = TP / (TP + FN)
```

**Interpretaci√≥n:** De los realmente positivos, ¬øcu√°ntos detectamos?

**Uso:** Cuando false negatives son costosos

**Ejemplo:** Diagn√≥stico m√©dico (no quieres perder casos positivos)

### F1-Score

M√©dia arm√≥nica de Precision y Recall:

```
F1 = 2 √ó (Precision √ó Recall) / (Precision + Recall)
```

**Uso:** Balance entre Precision y Recall

**Variantes:**
- **F0.5:** Favorece Precision
- **F2:** Favorece Recall

### Specificity

```
Specificity = TN / (TN + FP)
```

**Interpretaci√≥n:** De los negativos reales, ¬øcu√°ntos clasificamos correctamente?

### ROC Curve (Receiver Operating Characteristic)

Gr√°fico de True Positive Rate vs False Positive Rate en diferentes umbrales.

```
TPR = Recall = TP / (TP + FN)
FPR = FP / (FP + TN)
```

**AUC (Area Under Curve):**
- AUC = 1.0: Clasificador perfecto
- AUC = 0.5: Aleatorio
- AUC < 0.5: Peor que aleatorio

**Uso:** Comparar modelos, elegir threshold

### Precision-Recall Curve

Alternativa a ROC para clases desbalanceadas.

**Ventaja:** M√°s informativa cuando clase positiva es rara.

### Cohen's Kappa

Acuerdo considerando probabilidad de acuerdo aleatorio:

```
Œ∫ = (po - pe) / (1 - pe)
```

**Interpretaci√≥n:**
- Œ∫ = 1: Acuerdo perfecto
- Œ∫ = 0: Acuerdo aleatorio
- Œ∫ < 0: Peor que aleatorio

### Matthews Correlation Coefficient (MCC)

```
MCC = (TP√óTN - FP√óFN) / ‚àö((TP+FP)(TP+FN)(TN+FP)(TN+FN))
```

**Rango:** -1 a +1
**Ventaja:** Considera las 4 categor√≠as de confusi√≥n

## M√©tricas de Regresi√≥n

### Mean Absolute Error (MAE)

```
MAE = (1/n) Œ£|yi - ≈∑i|
```

**Ventajas:**
- F√°cil de interpretar
- Mismas unidades que variable objetivo
- Robusto a outliers

### Mean Squared Error (MSE)

```
MSE = (1/n) Œ£(yi - ≈∑i)¬≤
```

**Ventajas:**
- Penaliza errores grandes
- Diferenciable

**Desventaja:**
- Sensible a outliers
- Unidades al cuadrado

### Root Mean Squared Error (RMSE)

```
RMSE = ‚àöMSE
```

**Ventaja:** Mismas unidades que objetivo

### Mean Absolute Percentage Error (MAPE)

```
MAPE = (100/n) Œ£|yi - ≈∑i| / |yi|
```

**Ventaja:** Escala relativa (%)
**Desventaja:** Indefinido si yi = 0

### R¬≤ (Coeficiente de Determinaci√≥n)

```
R¬≤ = 1 - (SSres / SStot)
```

**Interpretaci√≥n:**
- R¬≤ = 1: Predicci√≥n perfecta
- R¬≤ = 0: Modelo tan bueno como la media
- R¬≤ < 0: Peor que la media

### Adjusted R¬≤

Penaliza por n√∫mero de features:

```
R¬≤_adj = 1 - [(1-R¬≤)(n-1) / (n-p-1)]
```

## Validaci√≥n de Modelos

### Train-Test Split

Divisi√≥n simple:
- 70-80% entrenamiento
- 20-30% prueba

**Ventaja:** Simple y r√°pido
**Desventaja:** Dependiente de split espec√≠fico

### Cross-Validation (K-Fold)

1. Dividir datos en K folds
2. Entrenar en K-1 folds
3. Evaluar en fold restante
4. Repetir K veces
5. Promediar resultados

**K t√≠pico:** 5 o 10

**Ventaja:** Usa todos los datos, m√°s robusto

### Stratified K-Fold

Mantiene proporci√≥n de clases en cada fold.

**Uso:** Datasets desbalanceados

### Leave-One-Out Cross-Validation (LOOCV)

K-Fold donde K = n (n√∫mero de observaciones)

**Uso:** Datasets muy peque√±os
**Desventaja:** Computacionalmente costoso

### Time Series Cross-Validation

Para datos temporales, mantener orden:
- Entrenar: t1 a tn
- Validar: tn+1 a tn+m
- Expandir ventana progresivamente

## Diagn√≥stico de Modelos

### Bias vs Variance

**High Bias (Underfitting):**
- Train error alto
- Test error alto
- Gap peque√±o

**High Variance (Overfitting):**
- Train error bajo
- Test error alto
- Gap grande

### Learning Curves

Gr√°fico de error vs tama√±o de training set.

**Diagn√≥stico:**
- Convergen en valor alto ‚Üí High bias
- Gap grande ‚Üí High variance
- Convergen en valor bajo ‚Üí Buen fit

### Residual Analysis

Para regresi√≥n:
- Plot residuos vs predicciones
- Patr√≥n aleatorio ‚Üí Buen modelo
- Patr√≥n sistem√°tico ‚Üí Problemas

## Selecci√≥n de Modelo

### Occam's Razor

Entre modelos con performance similar, elegir el m√°s simple.

### AIC (Akaike Information Criterion)

```
AIC = 2k - 2ln(L)
```

k = n√∫mero de par√°metros
L = likelihood

**Menor AIC es mejor**

### BIC (Bayesian Information Criterion)

```
BIC = k ln(n) - 2ln(L)
```

**Penaliza m√°s la complejidad que AIC**

## Comparaci√≥n de Modelos

### Statistical Tests

**Paired t-test:**
Comparar dos modelos en mismo dataset.

**McNemar's Test:**
Para clasificadores en same test set.

**Friedman Test:**
Comparar m√∫ltiples modelos en m√∫ltiples datasets.

### Confidence Intervals

Reportar m√©trica ¬± intervalo de confianza.

## Mejores Pr√°cticas

1. **Separar Test Set:** No tocarlo hasta el final
2. **M√∫ltiples M√©tricas:** No solo accuracy
3. **Cross-Validation:** Para modelos finales
4. **Baseline:** Comparar con modelo simple
5. **Reproducibilidad:** Fijar random seeds
6. **An√°lisis de Errores:** Entender qu√© falla
7. **Domain Knowledge:** Considerar costo de errores

## Errores Comunes

‚ùå **Data Leakage:** Informaci√≥n de test en train
‚ùå **Look-ahead Bias:** Usar datos futuros
‚ùå **Selection Bias:** Test set no representativo
‚ùå **Overfitting to Validation:** Tunear demasiado
‚ùå **Ignorar Contexto:** Usar m√©trica incorrecta

## Herramientas

### Python
```python
from sklearn.metrics import *

# Clasificaci√≥n
accuracy = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred)
recall = recall_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred)
cm = confusion_matrix(y_true, y_pred)
roc_auc = roc_auc_score(y_true, y_proba)

# Regresi√≥n
mae = mean_absolute_error(y_true, y_pred)
mse = mean_squared_error(y_true, y_pred)
rmse = mean_squared_error(y_true, y_pred, squared=False)
r2 = r2_score(y_true, y_pred)

# Cross-validation
from sklearn.model_selection import cross_val_score
scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')
```

## Juegos Relacionados

üéÆ [Clasificador Visual](/game/visual-classifier) - Observa m√©tricas de clasificaci√≥n

üéÆ [Overfitting Game](/game/overfitting-game) - Explora train vs test error

üéÆ [Regresi√≥n Lineal Builder](/game/linear-regression) - Visualiza R¬≤ y m√©tricas de regresi√≥n

## Recursos Adicionales

- Scikit-learn Metrics Guide
- Model Evaluation, Model Selection, and Algorithm Selection (Sebastian Raschka)
- Cross-validation: evaluating estimator performance
- Imbalanced-learn: M√©tricas para datos desbalanceados

---

*Anterior: [Redes Neuronales](/wiki/redes-neuronales) | Siguiente: [Cross-Validation](/wiki/cross-validation)*
