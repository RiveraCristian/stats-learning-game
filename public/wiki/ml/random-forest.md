# Random Forest

## Introducci√≥n

**Random Forest** es un algoritmo de ensemble que combina m√∫ltiples √°rboles de decisi√≥n para crear un modelo m√°s robusto y preciso. Cada √°rbol se entrena con una muestra aleatoria diferente de los datos y features, y las predicciones finales se obtienen por votaci√≥n (clasificaci√≥n) o promedio (regresi√≥n).

## Conceptos Fundamentales

### Ensemble Learning

Combinar m√∫ltiples modelos d√©biles para crear un modelo fuerte.

**Tipos:**
- **Bagging:** Bootstrap Aggregating
- **Boosting:** Secuencial, corrige errores
- **Stacking:** Combina diferentes tipos de modelos

Random Forest usa **Bagging**.

## C√≥mo Funciona Random Forest

### Proceso de Entrenamiento

1. **Bootstrap Sampling:** Crear n muestras aleatorias con reemplazo
2. **Random Feature Selection:** En cada split, considerar solo subset aleatorio de features
3. **Entrenar √°rboles:** Cada √°rbol crece sin poda
4. **Almacenar bosque:** Guardar todos los √°rboles

### Predicci√≥n

**Clasificaci√≥n:**
```
Predicci√≥n = Moda(predicciones de todos los √°rboles)
```

**Regresi√≥n:**
```
Predicci√≥n = Promedio(predicciones de todos los √°rboles)
```

## Ventajas de Random Forest

‚úÖ **Alta precisi√≥n:** Generalmente supera a √°rboles individuales
‚úÖ **Robusto al overfitting:** Promediado reduce varianza
‚úÖ **Feature importance:** Mide importancia de variables
‚úÖ **Maneja datos faltantes:** Autom√°ticamente
‚úÖ **Paralelizable:** √Årboles independientes
‚úÖ **Vers√°til:** Clasificaci√≥n y regresi√≥n

## Hiperpar√°metros Clave

**Del bosque:**
- **n_estimators:** N√∫mero de √°rboles (100-1000)
- **max_features:** Features por split ('sqrt', 'log2', n√∫mero)
- **bootstrap:** Usar bootstrap o no
- **oob_score:** Out-of-bag evaluation

**De √°rboles individuales:**
- **max_depth:** Profundidad m√°xima
- **min_samples_split:** M√≠nimo para dividir
- **min_samples_leaf:** M√≠nimo en hojas

## Feature Importance

### Mean Decrease in Impurity

Promedio de reducci√≥n de impureza al usar feature.

### Permutation Importance

Reducci√≥n en precisi√≥n al permutar feature aleatoriamente.

## Out-of-Bag (OOB) Error

Cada √°rbol se prueba con datos no usados en su entrenamiento (~37%).

**Ventaja:** Estimaci√≥n sin necesidad de conjunto de validaci√≥n separado.

## Aplicaciones

### Clasificaci√≥n
- Detecci√≥n de fraude
- Diagn√≥stico m√©dico
- Reconocimiento de im√°genes
- An√°lisis de sentimientos

### Regresi√≥n
- Predicci√≥n de precios
- Demanda de productos
- Series temporales
- Valoraci√≥n de propiedades

### Feature Selection
- Identificar variables importantes
- Reducir dimensionalidad

## Comparaci√≥n con Otros Algoritmos

| Aspecto | Random Forest | Gradient Boosting | Redes Neuronales |
|---------|---------------|-------------------|------------------|
| Precisi√≥n | Alta | Muy Alta | Muy Alta |
| Velocidad | R√°pido | Moderado | Lento |
| Interpretabilidad | Media | Media | Baja |
| Hiperpar√°metros | Pocos | Muchos | Muchos |
| Overfitting | Robusto | Propenso | Propenso |

## Implementaci√≥n

### Python (Scikit-learn)
```python
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    min_samples_split=2,
    max_features='sqrt',
    random_state=42
)

rf.fit(X_train, y_train)
predictions = rf.predict(X_test)

# Feature importance
importances = rf.feature_importances_
```

## Mejores Pr√°cticas

1. **M√°s √°rboles mejor:** 100-1000 t√≠picamente
2. **Tunear max_features:** sqrt para clasificaci√≥n, n/3 para regresi√≥n
3. **Cross-validation:** Validar hiperpar√°metros
4. **OOB score:** Usar cuando datos son limitados
5. **Profundidad:** Controlar si hay overfitting

## Limitaciones

‚ùå **Menos interpretable** que √°rbol √∫nico
‚ùå **Memoria:** Almacena muchos √°rboles
‚ùå **Predicci√≥n lenta:** M√∫ltiples √°rboles
‚ùå **No extrapolaci√≥n:** Solo interpola

## Variantes

- **Extra Trees:** Splits m√°s aleatorios, m√°s r√°pido
- **Isolation Forest:** Para detecci√≥n de anomal√≠as
- **Conditional Random Forest:** Maneja correlaci√≥n

## Juegos Relacionados

üéÆ [Clasificador Visual](/game/visual-classifier) - Compara decisiones de clasificadores

üéÆ [Overfitting Game](/game/overfitting-game) - Observa c√≥mo ensemble reduce overfitting

---

*Anterior: [√Årboles de Decisi√≥n](/wiki/arboles-decision) | Siguiente: [Redes Neuronales](/wiki/redes-neuronales)*
