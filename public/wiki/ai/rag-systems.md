# RAG Systems

## Introducci√≥n

**RAG (Retrieval-Augmented Generation)** es una t√©cnica revolucionaria que combina la b√∫squeda de informaci√≥n con la generaci√≥n de texto. Permite a los modelos de lenguaje acceder a conocimiento externo actualizado sin necesidad de reentrenamiento.

## ¬øQu√© es RAG?

RAG es una arquitectura que mejora los LLMs (Large Language Models) al darles acceso a una base de conocimientos externa.

### Componentes Principales

```
Query ‚Üí [Retrieval] ‚Üí Documentos Relevantes ‚Üí [LLM] ‚Üí Respuesta
```

1. **Retrieval**: Buscar informaci√≥n relevante
2. **Augmentation**: Agregar contexto al prompt
3. **Generation**: Generar respuesta basada en contexto

## ¬øPor Qu√© RAG?

### Problemas de LLMs Tradicionales

#### 1. Conocimiento Limitado
- Entrenados hasta una fecha de corte
- No saben eventos recientes
- **Ejemplo**: GPT-4 no sabe noticias de hoy

#### 2. Alucinaciones
- Inventan informaci√≥n que suena plausible
- No pueden verificar hechos
- **Riesgo**: Informaci√≥n incorrecta presentada con confianza

#### 3. Conocimiento Espec√≠fico
- No tienen acceso a datos propietarios
- No conocen documentos internos de empresas
- **Limitaci√≥n**: No pueden responder sobre tu base de datos

#### 4. Costo de Actualizaci√≥n
- Reentrenar es extremadamente caro
- Requiere millones de d√≥lares y meses
- **Problema**: Conocimiento queda obsoleto

### Soluci√≥n: RAG

‚úÖ **Conocimiento actualizado**: Acceso a informaci√≥n reciente
‚úÖ **Reducci√≥n de alucinaciones**: Respuestas basadas en fuentes
‚úÖ **Conocimiento espec√≠fico**: Puede usar tus documentos
‚úÖ **Costo-efectivo**: No requiere reentrenamiento
‚úÖ **Transparencia**: Puedes ver las fuentes

## Arquitectura de RAG

### Pipeline Completo

```
1. Indexaci√≥n (Offline)
   Documentos ‚Üí Chunking ‚Üí Embeddings ‚Üí Vector DB

2. Retrieval (Online)
   Query ‚Üí Embedding ‚Üí B√∫squeda Sem√°ntica ‚Üí Top-K Docs

3. Generation (Online)
   Query + Docs ‚Üí Prompt ‚Üí LLM ‚Üí Respuesta
```

### Paso 1: Indexaci√≥n

#### Chunking
Dividir documentos en fragmentos manejables.

**Estrategias**:
- **Fixed-size**: 512 tokens por chunk
- **Sentence-based**: Por oraciones completas
- **Semantic**: Por temas/secciones

**Ejemplo**:
```
Documento largo (10,000 palabras)
    ‚Üì
Chunks de 500 palabras
    ‚Üì
20 chunks indexados
```

#### Embeddings
Convertir texto a vectores num√©ricos.

**Modelos comunes**:
- OpenAI `text-embedding-ada-002`
- Sentence-BERT
- Cohere Embed

**Resultado**:
```
"La IA es fascinante" ‚Üí [0.23, -0.45, 0.67, ..., 0.12]
                         (vector de 1536 dimensiones)
```

#### Vector Database
Almacenar embeddings para b√∫squeda r√°pida.

**Opciones populares**:
- **Pinecone**: Managed, escalable
- **Weaviate**: Open-source, flexible
- **Chroma**: Simple, local
- **FAISS**: Facebook, muy r√°pido

### Paso 2: Retrieval

#### B√∫squeda Sem√°ntica

1. **Convertir query a embedding**
```
Query: "¬øC√≥mo funciona RAG?"
    ‚Üì
Embedding: [0.21, -0.43, 0.69, ...]
```

2. **Calcular similitud**
```
Similitud Coseno = (A ¬∑ B) / (||A|| √ó ||B||)
```

3. **Retornar Top-K m√°s similares**
```
Top 3 chunks m√°s relevantes
```

#### M√©todos de B√∫squeda

**Dense Retrieval**:
- Basado en embeddings
- Captura significado sem√°ntico
- **Ejemplo**: "auto" encuentra "coche", "veh√≠culo"

**Sparse Retrieval** (BM25):
- Basado en palabras clave
- B√∫squeda tradicional
- **Ejemplo**: "auto" solo encuentra "auto"

**Hybrid**:
- Combina dense + sparse
- Mejor de ambos mundos

### Paso 3: Generation

#### Prompt Augmentation

Construir prompt con contexto:

```
Contexto:
[Documento 1]: RAG combina retrieval y generation...
[Documento 2]: Los embeddings son representaciones vectoriales...
[Documento 3]: Vector databases permiten b√∫squeda r√°pida...

Pregunta: ¬øC√≥mo funciona RAG?

Responde bas√°ndote SOLO en el contexto proporcionado.
```

#### Generaci√≥n

El LLM genera respuesta usando el contexto:

```
RAG funciona en tres pasos: primero, convierte documentos 
en embeddings y los almacena en una vector database. Segundo, 
cuando llega una pregunta, busca los documentos m√°s relevantes. 
Tercero, usa esos documentos como contexto para generar una 
respuesta precisa.
```

## Ejemplo Pr√°ctico

### Caso de Uso: Chatbot de Soporte T√©cnico

**Problema**: Empresa tiene 1000 documentos de soporte.

**Soluci√≥n RAG**:

1. **Indexaci√≥n**:
```python
docs = load_documents("support_docs/")
chunks = split_into_chunks(docs, size=500)
embeddings = embed_chunks(chunks)
vector_db.store(embeddings, chunks)
```

2. **Query del Usuario**:
```
"¬øC√≥mo reseteo mi contrase√±a?"
```

3. **Retrieval**:
```python
query_embedding = embed("¬øC√≥mo reseteo mi contrase√±a?")
relevant_docs = vector_db.search(query_embedding, top_k=3)
```

4. **Generation**:
```python
prompt = f"""
Contexto: {relevant_docs}
Pregunta: ¬øC√≥mo reseteo mi contrase√±a?
Responde bas√°ndote en el contexto.
"""
response = llm.generate(prompt)
```

5. **Respuesta**:
```
Para resetear tu contrase√±a:
1. Ve a la p√°gina de login
2. Haz clic en "Olvid√© mi contrase√±a"
3. Ingresa tu email
4. Sigue las instrucciones del correo

[Fuente: Manual de Usuario, p.15]
```

## Ventajas de RAG

### vs Fine-Tuning

| Aspecto | RAG | Fine-Tuning |
|---------|-----|-------------|
| **Costo** | Bajo | Alto ($$$) |
| **Tiempo** | Minutos | D√≠as/Semanas |
| **Actualizaci√≥n** | Instant√°nea | Requiere reentrenar |
| **Transparencia** | Alta (cita fuentes) | Baja (caja negra) |
| **Datos necesarios** | Cualquier cantidad | Miles de ejemplos |
| **Conocimiento** | Expl√≠cito | Impl√≠cito |

### Casos de Uso Ideales

‚úÖ **Documentaci√≥n t√©cnica**: Manuales, APIs, gu√≠as
‚úÖ **Knowledge bases**: FAQs, wikis internas
‚úÖ **Investigaci√≥n**: Papers, art√≠culos cient√≠ficos
‚úÖ **Legal**: Contratos, regulaciones, casos
‚úÖ **M√©dico**: Historiales, estudios, protocolos
‚úÖ **Noticias**: Informaci√≥n actualizada

## Desaf√≠os y Soluciones

### 1. Calidad de Retrieval

**Problema**: Recuperar documentos irrelevantes.

**Soluciones**:
- Mejorar chunking strategy
- Usar hybrid search
- Reranking con modelo especializado
- Query expansion

### 2. Contexto Limitado

**Problema**: LLMs tienen l√≠mite de tokens (ej: 4K, 8K).

**Soluciones**:
- Chunks m√°s peque√±os
- Summarizaci√≥n de documentos largos
- Hierarchical retrieval
- Usar modelos con contexto largo (100K+)

### 3. Alucinaciones Persistentes

**Problema**: LLM inventa informaci√≥n no en el contexto.

**Soluciones**:
- Prompt engineering: "SOLO usa el contexto"
- Verificaci√≥n de citas
- Confidence scoring
- Human-in-the-loop

### 4. Latencia

**Problema**: Retrieval + Generation es lento.

**Soluciones**:
- Caching de queries comunes
- Async retrieval
- Optimizar vector DB
- Modelos m√°s r√°pidos

## T√©cnicas Avanzadas

### 1. Multi-Query RAG

Generar m√∫ltiples queries para mejor cobertura:

```
Query original: "¬øQu√© es RAG?"

Queries generadas:
- "Definici√≥n de RAG"
- "C√≥mo funciona Retrieval-Augmented Generation"
- "Componentes de sistemas RAG"
```

### 2. Hypothetical Document Embeddings (HyDE)

Generar respuesta hipot√©tica, luego buscar:

```
Query: "¬øC√≥mo funciona RAG?"
    ‚Üì
LLM genera respuesta hipot√©tica
    ‚Üì
Buscar documentos similares a la respuesta
```

### 3. Self-RAG

El modelo decide cu√°ndo necesita retrieval:

```
Query ‚Üí LLM decide si necesita contexto
    ‚Üì
Si s√≠ ‚Üí Retrieval ‚Üí Generation
Si no ‚Üí Generation directa
```

### 4. Iterative RAG

M√∫ltiples rondas de retrieval:

```
Query ‚Üí Retrieval ‚Üí Partial Answer
    ‚Üì
Refine Query ‚Üí Retrieval ‚Üí Better Answer
    ‚Üì
Repeat until satisfactory
```

## Evaluaci√≥n de RAG

### M√©tricas de Retrieval

**Recall@K**: ¬øRecuperamos documentos relevantes?
```
Recall@3 = Docs relevantes en top-3 / Total docs relevantes
```

**Precision@K**: ¬øLos recuperados son relevantes?
```
Precision@3 = Docs relevantes en top-3 / 3
```

**MRR (Mean Reciprocal Rank)**: Posici√≥n del primer relevante

### M√©tricas de Generation

**Faithfulness**: ¬øLa respuesta es fiel al contexto?

**Answer Relevance**: ¬øResponde la pregunta?

**Context Relevance**: ¬øEl contexto es relevante?

## Herramientas y Frameworks

### LangChain
Framework popular para RAG:
```python
from langchain import RetrievalQA

qa = RetrievalQA.from_chain_type(
    llm=OpenAI(),
    retriever=vector_store.as_retriever()
)
```

### LlamaIndex
Especializado en RAG:
```python
from llama_index import VectorStoreIndex

index = VectorStoreIndex.from_documents(docs)
query_engine = index.as_query_engine()
```

### Haystack
Pipeline flexible:
```python
from haystack import Pipeline

pipeline = Pipeline()
pipeline.add_node(retriever)
pipeline.add_node(generator)
```

## Casos de √âxito

### 1. Notion AI
- RAG sobre documentos del workspace
- Responde preguntas sobre tus notas
- Cita fuentes espec√≠ficas

### 2. GitHub Copilot Chat
- RAG sobre documentaci√≥n de c√≥digo
- Contexto de tu repositorio
- Sugerencias basadas en tu c√≥digo

### 3. Perplexity AI
- RAG sobre internet en tiempo real
- Cita fuentes web
- Informaci√≥n actualizada

## Futuro de RAG

### Tendencias

**Multimodal RAG**:
- Retrieval de im√°genes, videos, audio
- Generaci√≥n multimodal

**Agentic RAG**:
- Agentes que deciden qu√© recuperar
- M√∫ltiples fuentes de datos
- Razonamiento complejo

**Personalized RAG**:
- Adaptado a cada usuario
- Aprende preferencias
- Contexto hist√≥rico

## Conclusi√≥n

RAG es:
- üöÄ El futuro de LLMs con conocimiento actualizado
- üí° M√°s pr√°ctico que fine-tuning para muchos casos
- üéØ Esencial para aplicaciones empresariales
- üîÑ En constante evoluci√≥n

**Cu√°ndo usar RAG**:
- ‚úÖ Necesitas conocimiento actualizado
- ‚úÖ Tienes documentos espec√≠ficos
- ‚úÖ Quieres transparencia (citas)
- ‚úÖ Presupuesto limitado

**Cu√°ndo NO usar RAG**:
- ‚ùå Necesitas cambiar comportamiento del modelo
- ‚ùå Tarea no requiere conocimiento externo
- ‚ùå Latencia es cr√≠tica

---

*Siguiente: [Agentes Aut√≥nomos](/wiki/agentes-autonomos)*
