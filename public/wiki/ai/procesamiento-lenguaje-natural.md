# Procesamiento de Lenguaje Natural (NLP)

## Introducci√≥n

El **Procesamiento de Lenguaje Natural** (NLP) es el campo de la IA que permite a las m√°quinas entender, interpretar y generar lenguaje humano. Desde traductores autom√°ticos hasta chatbots y asistentes virtuales, NLP est√° transformando c√≥mo interactuamos con la tecnolog√≠a. Es uno de los campos m√°s activos y de r√°pido avance en IA.

## Conceptos Fundamentales

### ¬øQu√© es NLP?

Disciplina que combina:
- Ling√º√≠stica computacional
- Machine Learning
- Deep Learning
- Ciencias cognitivas

**Objetivo:** Cerrar la brecha entre lenguaje humano y comprensi√≥n computacional.

### Desaf√≠os del Lenguaje

**Ambig√ºedad:**
- "Vi al hombre con el telescopio" (¬øqui√©n tiene el telescopio?)

**Contexto:**
- "El banco est√° cerrado" (¬øinstituci√≥n financiera o asiento?)

**Variabilidad:**
- M√∫ltiples formas de expresar la misma idea

**Impl√≠cito:**
- Sarcasmo, iron√≠a, referencias culturales

**Evoluci√≥n:**
- Nuevas palabras, slang, modismos

## Niveles de An√°lisis

### 1. Fon√©tica y Fonolog√≠a

Estudio de sonidos del habla.

**Aplicaci√≥n:** Reconocimiento de voz

### 2. Morfolog√≠a

Estructura de palabras.

**Tareas:**
- Stemming: "corriendo" ‚Üí "corr"
- Lemmatization: "corriendo" ‚Üí "correr"

### 3. Sintaxis

Estructura gramatical de oraciones.

**Tareas:**
- Part-of-Speech (POS) Tagging
- Parsing: √Årboles sint√°cticos

### 4. Sem√°ntica

Significado de palabras y oraciones.

**Tareas:**
- Word Sense Disambiguation
- Named Entity Recognition (NER)

### 5. Pragm√°tica

Uso del lenguaje en contexto.

**Aspectos:**
- Intenci√≥n del hablante
- Conocimiento compartido
- Implicaturas

### 6. Discurso

An√°lisis m√°s all√° de oraciones individuales.

**Tareas:**
- Coreference Resolution
- Discourse Parsing

## Tareas Principales de NLP

### Text Classification

Categorizar textos.

**Ejemplos:**
- An√°lisis de sentimientos (positivo/negativo/neutral)
- Detecci√≥n de spam
- Clasificaci√≥n de noticias por tema
- Moderaci√≥n de contenido

### Named Entity Recognition (NER)

Identificar entidades en texto.

**Tipos:**
- Personas
- Lugares
- Organizaciones
- Fechas, cantidades

**Ejemplo:**
"Steve Jobs fund√≥ Apple en California"
- Steve Jobs: PERSONA
- Apple: ORGANIZACI√ìN
- California: LUGAR

### Machine Translation

Traducir entre idiomas autom√°ticamente.

**Evoluci√≥n:**
- Rule-based ‚Üí Statistical ‚Üí Neural MT
- Google Translate, DeepL

### Question Answering

Responder preguntas en lenguaje natural.

**Tipos:**
- Extractive: Extraer respuesta del texto
- Abstractive: Generar respuesta

**Ejemplos:** Siri, Alexa, ChatGPT

### Text Summarization

Generar res√∫menes de textos largos.

**Tipos:**
- Extractive: Seleccionar oraciones clave
- Abstractive: Generar nuevo texto

### Text Generation

Generar texto coherente.

**Aplicaciones:**
- Autocompletado
- Generaci√≥n de contenido
- Chatbots
- Escritura creativa

**Modelos:** GPT-3, GPT-4, Claude

### Information Extraction

Extraer informaci√≥n estructurada de texto no estructurado.

**Tareas:**
- Relaciones entre entidades
- Eventos
- Atributos

### Sentiment Analysis

Determinar emoci√≥n o opini√≥n en texto.

**Niveles:**
- Documento
- Oraci√≥n
- Aspecto (opiniones sobre caracter√≠sticas espec√≠ficas)

## T√©cnicas Tradicionales

### Bag of Words (BoW)

Representar documento como conjunto de palabras.

**Limitaciones:**
- Pierde orden
- Ignora contexto

### TF-IDF

Term Frequency - Inverse Document Frequency

```
TF-IDF(palabra) = TF √ó IDF
```

**Ventaja:** Da m√°s peso a palabras distintivas.

### N-grams

Secuencias de n palabras consecutivas.

**Ejemplos:**
- Unigram: "el", "gato"
- Bigram: "el gato"
- Trigram: "el gato negro"

### Regular Expressions

Patrones para buscar/extraer informaci√≥n.

**Uso:** Extracci√≥n de emails, tel√©fonos, fechas.

## Word Embeddings

Representaciones vectoriales densas de palabras que capturan significado sem√°ntico.

### Word2Vec

**Arquitecturas:**
- CBOW (Continuous Bag of Words)
- Skip-gram

**Idea:** Palabras similares tienen vectores similares.

**Ejemplo:**
```
king - man + woman ‚âà queen
```

### GloVe

Global Vectors for Word Representation.

Basado en matriz de co-ocurrencia de palabras.

### FastText

Extensi√≥n de Word2Vec considerando subwords.

**Ventaja:** Maneja palabras fuera de vocabulario.

## Modelos de Lenguaje

### RNN (Recurrent Neural Networks)

Procesan secuencias manteniendo estado oculto.

**Problema:** Vanishing gradient en secuencias largas.

### LSTM y GRU

Variantes de RNN que manejan mejor dependencias largas.

**Uso:** Traducci√≥n, generaci√≥n de texto

### Transformers

Arquitectura revolucionaria basada en mecanismo de atenci√≥n.

**Ventajas:**
- Paralelizable
- Captura dependencias largas
- State-of-the-art en NLP

**Componentes:**
- Self-Attention
- Multi-Head Attention
- Positional Encoding
- Feed-Forward Networks

## Modelos de Lenguaje Grandes (LLMs)

### BERT (Bidirectional Encoder Representations from Transformers)

**Caracter√≠sticas:**
- Bidireccional
- Pre-entrenado en masked language modeling
- Fine-tuning para tareas espec√≠ficas

**Uso:** Question answering, NER, classification

### GPT (Generative Pre-trained Transformer)

**Caracter√≠sticas:**
- Autoregresivo (genera palabra por palabra)
- Pre-entrenado en predicci√≥n de siguiente palabra
- Modelos: GPT-2, GPT-3, GPT-4

**Capacidades:**
- Generaci√≥n de texto
- Few-shot learning
- Zero-shot tasks

### T5 (Text-to-Text Transfer Transformer)

Todo como tarea text-to-text.

**Ejemplo:**
- Traducci√≥n: "translate English to Spanish: Hello" ‚Üí "Hola"

### Otros LLMs

- **PaLM (Google):** 540B par√°metros
- **LLaMA (Meta):** Open source
- **Claude (Anthropic):** Enfocado en seguridad
- **Gemini (Google):** Multimodal

## Tokenizaci√≥n

Dividir texto en unidades (tokens).

### Tipos

**Word Tokenization:**
```
"Hello world" ‚Üí ["Hello", "world"]
```

**Subword Tokenization (BPE, WordPiece):**
```
"unwrapping" ‚Üí ["un", "wrap", "ping"]
```

**Ventaja:** Vocabulario m√°s peque√±o, maneja palabras raras.

**Character Tokenization:**
```
"Hello" ‚Üí ["H", "e", "l", "l", "o"]
```

## Fine-Tuning

Adaptar modelo pre-entrenado a tarea espec√≠fica.

**Proceso:**
1. Cargar modelo pre-entrenado
2. Entrenar en dataset espec√≠fico
3. Ajustar solo √∫ltimas capas (o todas)

**Ventaja:** Requiere menos datos y tiempo.

## Prompt Engineering

Dise√±ar inputs para obtener mejores outputs de LLMs.

**T√©cnicas:**
- **Zero-shot:** Sin ejemplos
- **Few-shot:** Con ejemplos
- **Chain-of-thought:** Razonamiento paso a paso
- **Self-consistency:** M√∫ltiples generaciones

## Aplicaciones del Mundo Real

### Asistentes Virtuales

- Siri, Alexa, Google Assistant
- Entender comandos por voz
- Responder preguntas

### Chatbots

- Atenci√≥n al cliente
- Soporte t√©cnico
- Conversaciones generales (ChatGPT)

### Traducci√≥n

- Google Translate
- DeepL
- Traducci√≥n simult√°nea

### An√°lisis de Redes Sociales

- Sentiment analysis de tweets
- Detecci√≥n de tendencias
- Moderaci√≥n de contenido

### Salud

- Extracci√≥n de informaci√≥n de registros m√©dicos
- Asistentes para diagn√≥stico
- An√°lisis de literatura m√©dica

### Legal

- Revisi√≥n de contratos
- B√∫squeda de precedentes
- Redacci√≥n asistida

### Educaci√≥n

- Correcci√≥n autom√°tica
- Tutores virtuales
- Generaci√≥n de materiales

## Desaf√≠os Actuales

‚ùå **Bias:** Reproducir sesgos de datos de entrenamiento
‚ùå **Alucinaciones:** Generar informaci√≥n falsa con confianza
‚ùå **Idiomas de bajos recursos:** Pobre desempe√±o en idiomas menos comunes
‚ùå **Interpretabilidad:** Dif√≠cil entender decisiones
‚ùå **Contexto limitado:** Ventana de contexto finita
‚ùå **Costo computacional:** Entrenar LLMs es muy costoso

## M√©tricas de Evaluaci√≥n

### BLEU (Bilingual Evaluation Understudy)

Para traducci√≥n autom√°tica.

Compara n-grams con referencias.

### ROUGE (Recall-Oriented Understudy for Gisting Evaluation)

Para summarization.

Mide overlap con resumen de referencia.

### Perplexity

Para modelos de lenguaje.

Mide qu√© tan "sorprendido" est√° el modelo.

### Human Evaluation

A menudo necesaria para tareas generativas.

## Recursos y Herramientas

### Librer√≠as Python

**NLTK:**
- Natural Language Toolkit
- Herramientas cl√°sicas de NLP

**spaCy:**
- Industrial-strength NLP
- R√°pido y eficiente

**Hugging Face Transformers:**
- Acceso a modelos pre-entrenados
- Fine-tuning f√°cil

**Gensim:**
- Word embeddings
- Topic modeling

### Datasets

- **GLUE:** Benchmark de tareas de NLP
- **SQuAD:** Question answering
- **IMDb:** Sentiment analysis
- **Common Crawl:** Corpus web masivo

## Juegos Relacionados

üéÆ [NLP Challenge](/game/nlp-challenge) - Practica tokenizaci√≥n y an√°lisis de texto

üéÆ [Prompt Engineering](/game/prompt-engineering) - Mejora prompts para LLMs

## Recursos Adicionales

- Speech and Language Processing (Jurafsky & Martin)
- Natural Language Processing with Python (NLTK Book)
- Hugging Face Course
- fast.ai NLP
- Stanford CS224N: NLP with Deep Learning

---

*Anterior: [IA D√©bil vs Fuerte](/wiki/ia-debil-vs-fuerte) | Siguiente: [Fine-Tuning de Modelos](/wiki/fine-tuning-modelos)*
